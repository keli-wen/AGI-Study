![chat-llm-demo-header](chat-llm-demo-header.png)

# Chat-LLM DEMO V1

æˆ‘ä¸€ç›´å¸Œæœ›å†™ä¸€å†™æ¨¡å‹æ¨ç†ï¼Œéƒ¨ç½²å’Œ DEMO çš„å…¨æµç¨‹ã€‚å› ä¸ºä¸ªäººå­¦ä¹ æ—¶æ€»å¸Œæœ›èƒ½å¯¹æ•´ä¸ªæµç¨‹æœ‰ä¸€ä¸ªè®¤çŸ¥ã€‚å¦‚æœä»…ä»…æ˜¯è®©æ¨¡å‹è¿è¡Œèµ·æ¥å¾ˆç®€å•ï¼Œå¦‚ä½•åš DEMO ä¹Ÿæœ‰å¾ˆå¤šç²¾å“åšå®¢ã€‚è€Œæˆ‘çš„çˆ±å¥½æ˜¯åš minimum exampleï¼Œ**ç¡®ä¿æ¯è¡Œ code çš„å¿…è¦æ€§ï¼Œé¿å…ä¸å¿…è¦çš„ç‚«æŠ€**ï¼Œç¬¦åˆå½“å‰åšå®¢ä¸»é¢˜ï¼ˆå…¥é—¨ or æ·±å…¥ï¼‰ã€‚åŒæ—¶äº«å—è·‘é€š DEMO å¸¦æ¥çš„æ›´é«˜çš„æ»¡è¶³æ„Ÿã€‚

æœ¬ç¯‡ä¸»è¦ä½œä¸ºä¸€ä¸ª Baseï¼Œæ—¨åœ¨å…ˆä»å®æˆ˜å‡ºå‘ã€‚åç»­æˆ‘ä¼šåŸºäºæ­¤ç»™å‡ºï¼Œ`PyTriton`ï¼ˆéƒ¨ç½²ï¼‰ å’Œ `vLLM`ï¼ˆæ¨ç†ï¼‰ çš„ç›¸å…³åšå®¢ã€‚è™½ç„¶è¿™ä¸ª topic(chat-bot) æ¯”è¾ƒçƒ‚å¤§è¡—ï¼Œä½†æ˜¯èƒœåœ¨ç®€å•ï¼Œå®¹æ˜“å‡ºæ•ˆæœã€‚æˆ‘ä¼šå°½åŠ›ç”¨æœ€ç²¾ç®€çš„è¯­è¨€ï¼Œå¸¦å¤§å®¶äº†è§£ä¸€ä¸‹æ•´ä¸ªçš„ pipelineã€‚

åœ¨æˆ‘ä¹‹å‰å°è¯•åšä¸€ä¸ª Internal DEMO çš„æ—¶å€™ï¼Œæˆ‘å‘ç°æˆ‘å¯¹é™¤äº† Training ä¹‹å¤–çš„å†…å®¹äº†è§£æå°‘ï¼Œå¯¹äºå„ç§éƒ¨ç½²ï¼Œæ¨ç†æ–¹æ¡ˆæ›´æ˜¯ä¸€æ— æ‰€çŸ¥ã€‚æ£€ç´¢äº†ä¸€å †èµ„æ–™ï¼Œä¸œæ‹¼è¥¿å‡‘æ‰æœ‰äº†ä¸€ä¸ªå¤§æ¦‚çš„æ¦‚å¿µã€‚ç”±æ­¤å‘ç°ä¸­æ–‡äº’è”ç½‘ä¸Šè¿™æ–¹é¢å†…å®¹æœ‰æ‰€ç¼ºä¹ï¼Œå› æ­¤æˆ‘å°½åŠ›é€‰æ‹©äº†ä¸€äº›æ¯”è¾ƒçƒ­é—¨çš„å·¥å…·å’Œé¡¹ç›®ï¼ˆTriton Inference Server å’Œ vLLMï¼‰ï¼Œä¿è¯å†…å®¹æ—¶æ•ˆæ€§çš„åŒæ—¶å°½åŠ›è®©å¤§å®¶è„‘æµ·é‡Œæœ‰ä¸€ä¸ªç®€å•çš„è®¤çŸ¥é›å½¢ã€‚

æå‰å£°æ˜ï¼šæˆ‘çš„è¿™ç¯‡åšå®¢ä»…ä»…æ˜¯ä¸ºäº†è®©å¤§å®¶æœ‰ä¸€ä¸ªæ¦‚å¿µï¼Œçœ‹çœ‹å®é™…çš„ Code æ˜¯å¦‚ä½•ä¸²è”èµ·è¿™å‡ ä¸ªç¯èŠ‚çš„ï¼Œæ‰«ä¸€æ‰«ç›²ï¼Œå†…å®¹éå¸¸ç®€å•ï¼Œæ¬¢è¿å¤§å®¶è®¨è®ºã€‚

Example è®¡åˆ’ä¼šæœ‰ä¸¤ä¸ªç‰ˆæœ¬ï¼Œå†…å®¹åŒ…æ‹¬ï¼š
- [x] `transformers` for model.
- [x] `streamlit` for front-end.
- [x] `pytriton` for model deployment. (non-batch infer)
- [ ] `pytriton` for model deployment. (dynamic batching infer)
- [ ] `vLLM` for inference optimization. (maybe in the V2)
- [ ] Streaming ouput for better user experience. (maybe in the V2)


## Model Selection
æ¨¡å‹é€‰æ‹©çš„æ˜¯ `ğŸ³deepseek` çš„ [`llm-7b-chat`](https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat)ã€‚æˆ‘æœ¬åœ°è¿è¡Œå½“å‰é…ç½®å¤§çº¦éœ€è¦å•å¡ 15GB çš„æ˜¾å­˜ã€‚ï¼ˆä¸ªäººå»ºè®® 24GB åŠä»¥ä¸Šï¼‰


## Installation
å¦‚ä¸‹ä¸º Example æ‰€éœ€çš„ä¾èµ–ã€‚æ¬¢è¿å¤§å®¶è¿›è¡Œè¡¥å……ã€‚
> âš ï¸ å¦‚æœå­˜åœ¨ç½‘ç»œé—®é¢˜ï¼Œè¯·ä½¿ç”¨ `export HF_ENDPOINT=https://hf-mirror.com`ã€‚

```bash
# For front-end.
pip install streamlit

# For model.
pip install accelerate
pip install --upgrade transformers

# For NVIDIA-PyTriton.
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib
pip install -U nvidia-pytriton
```

## Running example locally
```bash
# You can omit it.
cd code-examples/chat-llm

# In one terminal. The CUDA_VISIBLE_DEVICES is optional.
CUDA_VISIBLE_DEVICES=7 python server.py

# In another terminal. 
streamlit run client.py --server.port 8080 --server.address 127.0.0.1
```

## Client ( frontend )

Client ç«¯ä¸»è¦è´Ÿè´£ UI å±•ç¤ºå’Œä¸ Server ç«¯çš„äº¤äº’ã€‚åŒ…æ‹¬ Streamlit å’Œ PyTriton Clientã€‚ç”±äºæ˜¯ Version1ï¼Œæ‰€ä»¥ä¸€åˆ‡ä»ç®€ï¼Œå¹¶æ²¡æœ‰å°è¯•é›†æˆè¯¸å¦‚ streaming ouput ç­‰ç‰¹æ€§ï¼Œé¿å…å¼•å…¥ä¸å¿…è¦çš„å›°æƒ‘ã€‚

### Streamlit

Chat-bot çš„é¡µé¢ä¸»è¦ä½¿ç”¨ Streamlit æ¡†æ¶çš„ç»„ä»¶å®ç°ã€‚æ•´ä½“å®ç°éå¸¸**ç®€æ´**ï¼Œå¦‚éœ€æ·±å…¥æ¨èé˜…è¯»è¯¥æ–‡æ¡£ [Streamlit: Build a basic LLM chat app](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps)ï¼Œå¤§çº¦éœ€è¦ `30` åˆ†é’Ÿã€‚

Streamlit æ˜¯ä¸€ä¸ªåŸºäº Python çš„å‰ç«¯æ¡†æ¶ï¼Œä¹Ÿæ˜¯ç®€æ´åœ°å®ç°è¯¥ DEMO çš„å…³é”®æ¡†æ¶ä¹‹ä¸€ã€‚ä¸ºé¿å…å¤§å®¶æ— æ³•ç†è§£ Streamlit çš„è¯­æ³•ï¼Œæˆ‘ä¼šç”¨æœ€ç®€å•ï¼Œæœ€ç›´æ¥çš„è¯­è¨€è¿›è¡Œä»‹ç»ã€‚

é¦–å…ˆï¼Œåªéœ€è¦ç†è§£ Streamlit çš„ä¸€ä¸ªç‰¹æ€§ã€‚å®ƒçš„ç‰¹ç‚¹æ˜¯ï¼š**æ¯æ¬¡ç”¨æˆ·çš„äº¤äº’éƒ½ä¼šè§¦å‘æ•´ä¸ªé¡µé¢çš„é‡æ–°æ¸²æŸ“**ã€‚ç®€å•æ¥è¯´ï¼šå®ƒæ˜¯ä»å¤´é‡æ–°æ‰§è¡Œæ•´ä¸ªè„šæœ¬æ¥è¿›è¡Œäº¤äº’çš„ã€‚**æ˜¾ç„¶ï¼Œå¦‚æœåªæ˜¯ä»å¤´æ‰§è¡Œè„šæœ¬ï¼Œé‚£ä¹ˆæ‰€æœ‰çš„çŠ¶æ€éƒ½ä¼šä¸¢å¤±**ï¼ˆä½ é‡æ–°æ‰§è¡Œåå¦‚ä½•çŸ¥é“ä¸Šä¸€ä¸ªäº¤äº’æ“ä½œæ˜¯ä»€ä¹ˆï¼Ÿï¼‰ã€‚Streamlit æä¾›ä¸€ä¸ª `st.session_state` å­—å…¸ç”¨äºæ¯æ¬¡é‡æ–°æ‰§è¡Œæ—¶çš„çŠ¶æ€ä¿æŒï¼ˆåªæœ‰è¿™é‡Œé¢çš„ä¿¡æ¯æ‰æ˜¯æŒä¹…åŒ–çš„ä¿¡æ¯ï¼‰ã€‚æ‹¿ Example Code ä¸¾ä¾‹ï¼š
- `Line 15`ï¼Œå¯¹äºä¸€äº›ç‰¹æ®Šçš„å¯¹è±¡ï¼Œè¯¸å¦‚ç½‘ç»œè¿æ¥ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›æ¯æ¬¡äº¤äº’æ—¶éœ€è¦é‡æ–°è¿æ¥ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬æŠŠ `ModelClient` åˆå§‹åŒ–åå­˜åœ¨ `st.session_state` ä¸­é¿å…é‡å¤æ„é€ ã€‚ï¼ˆå½“ç„¶ï¼ŒStreamlit ä¹Ÿæä¾›æ›´é«˜çº§çš„ `cache` æ–¹æ³•ï¼Œä½†æ˜¯è¯¥ç‰ˆè¿›è¡Œæœ€å°é™åº¦çš„å®ç°ï¼‰ã€‚
- `Line 24`ï¼Œ å¯¹äº chat-bot æˆ‘ä»¬éœ€è¦ä¿ç•™ç”¨æˆ·å’Œ bot çš„æ‰€æœ‰æ¶ˆæ¯è®°å½•ï¼Œæ‰€ä»¥åœ¨ `st.session_state` ä¸­ç”¨  `history_message` ä¿å­˜ã€‚

å¦‚æœä½ èƒ½ç†è§£ä¸Šé¢è¿™éƒ¨åˆ†ï¼Œé‚£ä¹ˆ DEMO è¿™å—å°†ä¸å†æœ‰ä»»ä½• blockã€‚ä»£ç ä¸­çš„æ³¨é‡Šåº”è¯¥å·²ç»è¶³å¤Ÿå¸®åŠ©ä½ ç†è§£ã€‚


### PyTriton Client

> âœ¨ PyTriton æ˜¯ NVIDIA å¯¹äº Triton Inference Server çš„çº¯ Python å®ç°ï¼Œç”¨æ¥ç®€åŒ– Python ç¯å¢ƒä¸‹çš„æ¨¡å‹éƒ¨ç½²ã€‚å¯ä»¥ç†è§£ä¸ºæä¾›æ¨¡å‹æ¨ç†çš„å‰åç«¯é…å¥—æ¥å£å³å¯ã€‚ä¸ªäººè¿˜æ˜¯æ¯”è¾ƒçœ‹å¥½ PyTriton çš„ï¼Œä½†æ˜¯ç›®å‰æ–‡æ¡£æœ‰ä¸€äº›ä¸å®Œå–„ï¼Œç¤¾åŒºèµ„æ–™ä¹Ÿè¾ƒå°‘ï¼ˆå› ä¸ºå¤ªæ–°äº†ï¼‰ï¼Œä¹‹åä¼šå°è¯•å¤šè®°å½•ä¸€äº› PyTriton çš„å†…å®¹ã€‚PyTriton çš„ä¼˜åŠ¿æˆ‘ä¼šåœ¨åç»­çš„åšå®¢ä¸­è¿›è¡Œåˆ†æï¼Œç°åœ¨ä½ åªéœ€è¦ç»§ç»­å®ç°ä¸‹å»ã€‚


PyTriton ä¸­æœ‰å¤šç§å¯ç”¨çš„ `Client`ï¼Œè¿™é‡Œé€‰æ‹©æœ€ç®€æ´çš„ `ModelClient`ã€‚åªéœ€è¦æˆ‘ä»¬ä¼ å…¥å¯¹åº”çš„ `url` å’Œ `model_name` å³å¯æ„é€ ã€‚è¿™é‡Œçš„ `url` å‰ç¼€ä¸º `grpc://`ï¼Œä»£è¡¨æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ `gRPC` åè®®ã€‚PyTriton æ”¯æŒ `HTTP` å’Œ `gRPC` ä¸¤ç§åè®®ï¼Œä½†æ˜¯ `gRPC` é€šå¸¸æ›´ç¨³å®šã€‚

ä» `Line 42` å¼€å§‹æ¨¡å‹æ¨ç†éƒ¨åˆ†ã€‚

`Step1` çœ‹èµ·æ¥æœ‰ä¸€äº›ä¸å¿…è¦çš„å†—é•¿ã€‚è¿™æ˜¯å› ä¸ºï¼š**PyTriton ä»…æ”¯æŒä¼ è¾“ `numpy.ndarray` ç±»å‹çš„æ•°æ®**ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦è¿›è¡Œä¸€äº›ç®€å•çš„ç¼–ç ä¸è½¬æ¢æ“ä½œã€‚ä½ å¯ä»¥ç•¥è¿‡è¿™éƒ¨åˆ†å¹¶å½“ä½œæ˜¯ä¸€ä¸ªé»‘ç›’å¤„ç†ã€‚

> ğŸ’ å¦‚æœä½ æƒ³äº†è§£æ›´å¤šï¼Œæˆ‘ç»™å‡ºæ¯ä¸ªæ“ä½œå¯¹åº”çš„ç±»å‹è½¬æ¢ï¼š
> ```python
> List[Dict[str, str]] -(json.dumps)-> str -(str.encode)> -> bytes -(np.frombuffer)-> np.ndarray(dtype=np.uint8)
> ```

`Step2` ä¸­æˆ‘ä»¬ä½¿ç”¨é¢„æ„é€ çš„ `client` å‘é€æ¨ç†è¯·æ±‚ï¼Œæ³¨æ„ `PyTriton` çš„ `client` æä¾›ä¸¤ç§æ¨ç†å‡½æ•°ï¼š`infer_sample` å’Œ `infer_batch`ï¼Œåˆ†åˆ«å¯¹åº”å•ä¸ªæ¨ç†å’Œæ‰¹å¤„ç†æ¨ç†ã€‚ä¸ºäº†é¿å…å¼•å…¥æ›´å¤šçš„å†…å®¹ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ `infer_sample` ä»£è¡¨æˆ‘ä»¬ä¼ å…¥çš„æ˜¯ä¸€ä¸ªå•ä¸ªï¼ˆè€Œéæ‰¹å¤„ç†ï¼‰çš„æ¨ç†è¯·æ±‚ã€‚

`Step3` ä¸­æˆ‘ä»¬å°†æ¨ç†ç»“æœè¿›è¡Œè§£ç ã€‚è¿™é‡Œçš„è§£ç æ“ä½œä¸ç¼–ç æ“ä½œç›¸å¯¹åº”ã€‚ä½ å¯ä»¥ç†è§£ä¸ºï¼š**æˆ‘ä»¬å°†æ¨ç†ç»“æœä» `numpy.ndarray` ç±»å‹è§£ç ä¸º `List[Dict[str, str]]` ç±»å‹**ã€‚

è‡³æ­¤ï¼Œæˆ‘ä»¬å·²ç»ç†è§£äº† Client ç«¯çš„æ‰€æœ‰ä»£ç ã€‚ä»…éœ€å‡ åè¡Œä»£ç ï¼Œæˆ‘ä»¬å°±å®Œæˆäº†ä¸€ä¸ªç®€å•çš„ chat-bot çš„å‰ç«¯å®ç°ã€‚

## Server ( backend )

Server ç«¯ä¸»è¦è´Ÿè´£æ¨¡å‹çš„åŠ è½½ä¸ client ç«¯æ¨ç†è¯·æ±‚çš„å“åº”ã€‚åŒ…æ‹¬ Huggingface Transformers å’Œ PyTriton Inference Serverã€‚

### Huggingface Transformers

å…³äºæ¨¡å‹çš„åŠ è½½éƒ¨åˆ†ï¼Œæˆ‘åŸºæœ¬å‚è€ƒäº† DeepSeek LLM HuggingFace é¡µé¢çš„[å®˜æ–¹å®ç°](https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat#chat-completion)ã€‚å®˜æ–¹å®ç°çš„ä»£ç ä¹Ÿå¯ä»¥æŠ½è±¡ä¸ºï¼š**åŠ è½½**å’Œ**æ¨ç†**ä¸¤ä¸ªé˜¶æ®µã€‚åŠ è½½çš„ä»£ç æ˜¯å®Œå…¨å¯é‡ç”¨çš„ï¼ˆåç»­é›†æˆ `vLLM` æ—¶å¯èƒ½éœ€è¦å˜åŒ–ï¼‰ã€‚ç›®å‰ï¼Œåªéœ€ä¿®æ”¹æ¨ç†ä»£ç ä»¥åŒ¹é… PyTriton çš„è¾“å…¥è¾“å‡ºå³å¯ã€‚

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig

# Sec1 Load.
model_name = "deepseek-ai/deepseek-llm-7b-chat"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name, torch_dtype=torch.bfloat16, device_map="auto"
)
model.generation_config = GenerationConfig.from_pretrained(model_name)
model.generation_config.pad_token_id = model.generation_config.eos_token_id

# Sec2 Infer.
messages = [{"role": "user", "content": "Who are you?"}]
input_tensor = tokenizer.apply_chat_template(
    messages, add_generation_prompt=True, return_tensors="pt"
)
outputs = model.generate(input_tensor.to(model.device), max_new_tokens=100)

result = tokenizer.decode(
    outputs[0][input_tensor.shape[1] :], skip_special_tokens=True
)
print(result)
```

### PyTriton Inference Server

PyTriton Server ç«¯çš„é…ç½®æµç¨‹ä¸»è¦åŒ…æ‹¬ï¼š

1. é…ç½® `TritonConfig` å¹¶å¯åŠ¨ `Triton`ã€‚(easy)
2. ğŸ”¥ ä½¿ç”¨ `Triton.bind()` é…ç½®æ¨¡å‹ä¿¡æ¯ï¼Œè¾“å…¥è¾“å‡ºä¸ºåº¦ï¼Œæ¨ç†è®¾ç½®ä¸ç»‘å®šæ¨ç†å‡½æ•° `_infer_fn`ã€‚ (key)
3. ä½¿ç”¨ `triton.server()` ä»¥ [Blocking mode](https://triton-inference-server.github.io/pytriton/0.5.3/initialization/#blocking-mode) å¯åŠ¨æœåŠ¡ã€‚ (easy)

è¿™ä¸‰æ­¥ä¸­ä»…æœ‰ç¬¬äºŒæ­¥éå¸¸å…³é”®ã€‚æˆ‘ä»¬é‡ç‚¹ç ”ç©¶ä¸€ä¸‹ `Triton.bind()`ã€‚ `Triton.bind()` åŒ…å«å¦‚ä¸‹[å‚æ•°](https://triton-inference-server.github.io/pytriton/0.5.3/reference/triton/#pytriton.triton.Triton.bind)ï¼š

- `model_name`ï¼šä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä»£è¡¨ Triton Inference Server ä¸­å¯ç”¨çš„ Model åï¼Œåœ¨å‘é€è¯·æ±‚æ—¶æœ‰ç”¨ã€‚
- `infer_func`ï¼šPython å‡½æ•°ï¼Œä»£è¡¨ Triton Inference Server è¯¥å¦‚ä½•å¤„ç†æ¨ç†è¯·æ±‚ã€‚
- `inputs`ï¼šå®šä¹‰æ¨¡å‹è¾“å…¥å‚æ•°çš„æ•°é‡ï¼Œç±»å‹å’Œå½¢çŠ¶ã€‚
- `outputs`ï¼šå®šä¹‰æ¨¡å‹è¾“å‡ºå‚æ•°çš„æ•°é‡ï¼Œç±»å‹å’Œå½¢çŠ¶ã€‚
- `config`ï¼šç”¨äºå¯¹ Triton Inference Server ä¸Šçš„æ¨¡å‹éƒ¨ç½²è¿›è¡Œæ›´å¤šå®šåˆ¶ï¼Œæ¯”å¦‚ `batching` å’Œ `max_batch_size` ç­‰ã€‚
- `strict`ï¼šå¯ç”¨å¯¹**æ¨ç†è¾“å‡º**çš„æ•°æ®ç±»å‹å’Œå½¢çŠ¶è¿›è¡ŒéªŒè¯ï¼Œä»¥ç¡®ä¿å…¶ç¬¦åˆæä¾›çš„æ¨¡å‹é…ç½®ï¼ˆé»˜è®¤ä¸ºï¼š`False`ï¼‰ã€‚

ç”±äºæˆ‘ä»¬æ—¨åœ¨å®ç°ä¸€ä¸ªæœ€ç®€åŒ–çš„ç‰ˆæœ¬ï¼Œå…¶ä»–çš„å‚æ•°åŸºæœ¬éƒ½æ˜¯é»˜è®¤å€¼æˆ–è€…æœ€ç®€æ˜“çš„å¡«å……ï¼ˆæ¯”å¦‚ `config` ä¸­çš„ `batching=False`ï¼‰ã€‚æˆ‘ä»¬æŠŠç›®å…‰èšç„¦äº `inputs`ï¼Œ `outputs` å’Œ `infer_func` è¿™å‡ ä¸ªä¸å…·ä½“åœºæ™¯ç›¸å…³çš„å‚æ•°ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦é’ˆå¯¹åœºæ™¯å®šä¹‰ `input` å’Œ `output` çš„ç»´åº¦ä¸æ•°æ®ç±»å‹ã€‚è¿™é‡Œæœ‰ä¸€äº›å°å‘ï¼ˆä¹Ÿæ˜¯ç›®å‰ PyTriton æ–‡æ¡£ä¸­ç•¥æœ‰æ¬ ç¼ºçš„åœ°æ–¹ï¼‰ã€‚

å¦‚æœæˆ‘ä»¬åœ¨ `config` ä¸­è®¾ç½®äº† `batching=True`ï¼Œé‚£ä¹ˆ `input` å’Œ `output` ä¼šé»˜è®¤æŠŠç¬¬ä¸€ç»´åº¦ä¸º `batch` ç»´åº¦ã€‚å¦åˆ™ä¸å­˜åœ¨ batch ç»´ï¼Œä¸¾å‡ ä¸ªä¾‹å­ï¼š

```python
# config.batching = True
# ä¸‹é¢çš„ input éœ€æ±‚çš„æ˜¯ä¸€ä¸ª [x, 1, 3] çš„è¾“å…¥ã€‚
inputs = [Tensor(name="input0", dtype=np.float32, shape=[1, 3])]

# config.batching = False
# ä¸‹é¢çš„ input éœ€æ±‚çš„æ˜¯ä¸€ä¸ª [1, 3] çš„è¾“å…¥ã€‚
inputs = [Tensor(name="input0", dtype=np.float32, shape=[1, 3])]
```

å¦‚æœä½¿ç”¨ `-1` åˆ™è¡¨ç¤º `input` æˆ– `output` çš„åŠ¨æ€å½¢çŠ¶ã€‚åœ¨è¿™é‡Œç”±äºæˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨ `batching`ï¼Œæ‰€ä»¥ `input` ä»…ä¸ºä¸€ç»´çš„ï¼Œå½¢å¦‚ `(x,)` çš„åºåˆ—ï¼Œè€Œ `output` åˆ™ä¸ºä¸€ä¸ªå½¢çŠ¶å›ºå®šçš„ `(1,)` åºåˆ—ã€‚

ä¹‹åï¼Œåªéœ€è¦å®ç°ä¸€ä¸ª `_infer_fn` å‡½æ•°å³å¯ã€‚è®°å¾—å‰ç«¯è¿›è¡Œçš„æ•°æ®è½¬æ¢æ“ä½œå˜›ï¼Ÿç°åœ¨éœ€è¦å…ˆè¿›è¡Œä¸€æ­¥é€†æ“ä½œè¿˜åŸå› `List[Dict[str, str]]` ç±»å‹çš„æ•°æ®ã€‚ä¹‹åä¾¿å¯ç›´æ¥å¤ç”¨ä¸Šæ–‡ä¸­ Huggingface Transformers æ®µä¸­ Sec2 çš„æ¨ç†ä»£ç ã€‚ä»ç„¶éœ€è¦æ³¨æ„çš„æ˜¯ï¼š**æˆ‘ä»¬çš„è¿”å›å€¼åº”ä¸º `numpy.ndarray` ç±»å‹**ã€‚

> â•è¿™é‡Œå¯èƒ½æœ‰äººä¼šæƒ³çŸ¥é“ä»£ç ä¸­çš„ `@sample` çš„ decorator æœ‰ä»€ä¹ˆä½œç”¨ã€‚
> 
> è¿™é‡Œå¦‚æœè¯¦ç»†åˆ†æå¯èƒ½ä¼šä¸å¿…è¦çš„å¤æ‚ï¼Œé€šå¸¸æƒ…å†µä¸‹æˆ‘ä»¬ä¼šä½¿ç”¨ä¸¤ç§é€‰æ‹©ï¼š
> - `@sample`, é€‚ç”¨äº non-batch æ¨ç†ï¼Œå³æ¯æ¬¡æ¨ç†è¯·æ±‚åªæœ‰ä¸€ä¸ªè¾“å…¥ã€‚
> - `@batch`, é€‚ç”¨äº batch æ¨ç†ï¼Œå³æ¯æ¬¡æ¨ç†è¯·æ±‚æœ‰å¤šä¸ªè¾“å…¥ã€‚
> 
> `@batch` çš„ä½¿ç”¨ä¼šå‡ºç°åœ¨ DEMO V2 ä¸­ï¼Œå¦‚æœä½ è¿˜å¸Œæœ›äº†è§£æ›´å¤šï¼Œæˆ‘ä¼šåœ¨ä¹‹åè¯¦ç»†ä»‹ç» PyTriton çš„åšå®¢ä¸­æåŠæˆ–ä½ å¯ä»¥å‚è€ƒ NVIDIA çš„å®˜æ–¹æ–‡æ¡£ï¼š[PyTriton: Decorators](https://triton-inference-server.github.io/pytriton/0.5.3/inference_callables/decorators/)ã€‚

```diff
@sample
def _infer_fn(messages: np.ndarray) -> np.ndarray:
-   messages = [
-       {"role": "user", "content": "Who are you?"}
-   ]
+   # Reverse the encoding operation.
+   messages = json.loads(messages.tobytes().decode("utf-8"))

    input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt")
    outputs = model.generate(input_tensor.to(model.device), max_new_tokens=100)

    result = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)
-   print(result)
+   # Convert the result to `np.ndarray` type.
+   return {"response": np.char.encode([result], "utf-8")}
```

## Summary and Future Work

è‡³æ­¤ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†ä¸€ä¸ªç®€å•çš„ chat-bot çš„å‰åç«¯å®ç°ã€‚è¯¥ä»£ç æ˜¯ Version1ï¼Œæ—¨åœ¨ä»¥æœ€å°çš„ example æ¥å±•ç¤ºæ•´ä¸ª DEMO æ¨ç†ä¸éƒ¨ç½²çš„æµç¨‹ï¼Œå¹¶ä¸æ¶‰åŠæ›´å¤šé«˜çº§ç‰¹æ€§ã€‚

åœ¨ Version2 ä¸­æˆ‘ä¼šè¿›è¡Œæ›´å¤šçš„æ‹“å±•ï¼Œè¡¥å……è¯¸å¦‚ `vLLM` çš„æ¨ç†ä¼˜åŒ–ï¼ŒPyTriton çš„ `Dynamic Batching` ä¸æ–‡æœ¬æµå¼è¾“å‡ºç­‰é«˜çº§ç‰¹æ€§ã€‚

æ›´è¿›ä¸€æ­¥ï¼Œæˆ‘ä¼šå°½åŠ›çš„åšå‡º PyTriton ç›¸å…³çš„ä¸“ä¸šæŠ€æœ¯åšå®¢ä¸ vLLM æºä»£ç åˆ†æã€‚å¸Œæœ›èƒ½å¸®åŠ©å¤§å®¶æ›´å¥½çš„ç†è§£æ¨¡å‹æ¨ç†ä¸éƒ¨ç½²çš„å…¨æµç¨‹ã€‚åœ¨ Applied AI ä¸­æ›´è¿›ä¸€æ­¥ã€‚

## Reference
- [ğŸ¤— Huggingface: deepseek-llm-7b-chat](https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat#chat-completion)
- [Streamlit: Build a basic LLM chat app](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps)
- [PyTriton: Decorators](https://triton-inference-server.github.io/pytriton/0.5.3/inference_callables/decorators/)
- [PyTriton: Blocking mode](https://triton-inference-server.github.io/pytriton/0.5.3/initialization/#blocking-mode)
- [PyTriton: Triton.bind()](https://triton-inference-server.github.io/pytriton/0.5.3/reference/triton/#pytriton.triton.Triton.bind)
- [NVIDIA: Triton Shape Tensors](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_configuration.html#shape-tensors)
