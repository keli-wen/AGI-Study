# vLLM Study

*--- vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.*

***v* means [virtual](https://github.com/vllm-project/vllm/issues/835).**

åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢ç´¢vLLMï¼Œè¿™æ˜¯ç›®å‰æœ€çƒ­é—¨çš„å‡ ä¸ªæ¨ç†åŠ é€Ÿå¼•æ“ä¹‹ä¸€ï¼Œå¹¶ä¸”å®ƒæ˜¯å®Œå…¨å¼€æºçš„ã€‚æœ¬ç¯‡å†…å®¹å°†åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼š

1. **vLLMçš„ä½¿ç”¨ä¸éƒ¨ç½²**ï¼šè¿™ä¸€éƒ¨åˆ†ä¸»è¦åŸºäºå®˜æ–¹æ–‡æ¡£ï¼Œæä¾›vLLMçš„ä½¿ç”¨å’Œéƒ¨ç½²æŒ‡å—ï¼Œæ—¨åœ¨ä¸ºç”¨æˆ·æä¾›æ¸…æ™°ã€ç›´æ¥çš„æ“ä½œæ­¥éª¤ã€‚
2. **vLLMæºç æ¢ç´¢**ï¼šåœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†æ·±å…¥vLLMçš„æºä»£ç ï¼Œåˆ†æå…¶å·¥ä½œåŸç†å’Œæ¶æ„ï¼Œä¸ºå¯¹æŠ€æœ¯ç»†èŠ‚æ„Ÿå…´è¶£çš„è¯»è€…æä¾›æ·±åº¦è§£æã€‚

## vLLM çš„ä½¿ç”¨/éƒ¨ç½²

vLLM æ˜¯ä¸€ä¸ªå¿«é€Ÿä¸”æ˜“äºä½¿ç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹(LLM)æ¨ç†å’ŒæœåŠ¡åº“ã€‚ä¸‹é¢æ˜¯ vLLM å¸¦æ¥çš„æ¨ç†é€Ÿåº¦çš„å¯è§†åŒ–ç»“æœã€‚

![vLLM-optimization-result](./assets/vLLM-optimization-result.png)

vLLM ä¹‹æ‰€ä»¥å¿«é€Ÿï¼Œä¸»è¦å¾—ç›Šäºï¼š

- **æœ€å…ˆè¿›çš„æœåŠ¡ååé‡**ï¼šæä¾›ä¸šç•Œé¢†å…ˆçš„å¤„ç†é€Ÿåº¦ã€‚
- **é«˜æ•ˆçš„æ³¨æ„åŠ›é”®å€¼å†…å­˜ç®¡ç†**ï¼šé€šè¿‡ PagedAttention æŠ€æœ¯å®ç°é«˜æ•ˆç®¡ç†ã€‚
- **è¿ç»­æ‰¹å¤„ç†æ¥è‡ªè¯·æ±‚**ï¼šèƒ½å¤Ÿè¿ç»­ä¸æ–­åœ°å¤„ç†è¾“å…¥è¯·æ±‚ã€‚
- **ä½¿ç”¨ CUDA/HIP å›¾çš„å¿«é€Ÿæ¨¡å‹æ‰§è¡Œ**ï¼šå€ŸåŠ© CUDA/HIP æŠ€æœ¯ï¼Œæé«˜æ¨¡å‹è¿è¡Œæ•ˆç‡ã€‚
- **é‡åŒ–æŠ€æœ¯**ï¼šåŒ…æ‹¬ GPTQã€AWQã€SqueezeLLM ç­‰å…ˆè¿›çš„é‡åŒ–æ–¹æ³•ã€‚
- **ä¼˜åŒ–çš„ CUDA æ ¸å¿ƒ**ï¼šé€šè¿‡ä¼˜åŒ– CUDA æ ¸å¿ƒæé«˜æ€§èƒ½ã€‚

vLLM åœ¨çµæ´»æ€§å’Œæ˜“ç”¨æ€§æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå…·ä½“ä½“ç°åœ¨ï¼š

- **ä¸æµè¡Œçš„ HuggingFace æ¨¡å‹æ— ç¼é›†æˆ**ï¼šæ–¹ä¾¿ç”¨æˆ·ä¸å½“å‰æµè¡Œçš„æ¨¡å‹è¿›è¡Œæ•´åˆã€‚
- **æ”¯æŒå¤šç§è§£ç ç®—æ³•çš„é«˜ååé‡æœåŠ¡**ï¼šåŒ…æ‹¬å¹¶è¡Œé‡‡æ ·ã€æŸæœç´¢ç­‰å¤šç§ç®—æ³•ã€‚
- **æ”¯æŒå¼ é‡å¹¶è¡Œæ€§çš„åˆ†å¸ƒå¼æ¨ç†**ï¼šé€‚ç”¨äºåˆ†å¸ƒå¼ç³»ç»Ÿçš„æ¨ç†è®¡ç®—ã€‚
- **æµå¼è¾“å‡º**ï¼šæ”¯æŒè¿ç»­æ•°æ®æµçš„è¾“å‡ºæ–¹å¼ã€‚
- **å…¼å®¹ OpenAI çš„ API æœåŠ¡å™¨**ï¼šæ˜“äºä¸ OpenAI ç”Ÿæ€ç³»ç»Ÿé›†æˆã€‚
- **æ”¯æŒ NVIDIA å’Œ AMD GPU**ï¼šå…¼å®¹ä¸»æµçš„ GPU ç¡¬ä»¶ã€‚

æ¥ä¸‹æ¥è¿›è¡Œæ›´ä¸ºè¯¦ç»†çš„ä½¿ç”¨ä»‹ç»ï¼Œä»å„ä¸ªè§’åº¦åˆ†æï¼švLLM å¯ç”¨æ€§ä¸ä½¿ç”¨æ€§ã€‚

### Install

You can install vLLM using pip:

```
$ # (Optional) Create a new conda environment.
$ conda create -n myenv python=3.9 -y
$ conda activate myenv

$ # Install vLLM with CUDA 12.1.
$ pip install vllm
```

### ğŸ”¥ Serve

vLLM éå¸¸æ–¹ä¾¿çš„æ”¯æŒç¦»çº¿çš„æ¨ç†ï¼Œä¹Ÿèƒ½å®ç°å®Œå…¨å…¼å®¹ OpenAI API çš„æœåŠ¡ã€‚ä¸‹é¢ä»‹ç»ä¸€ä¸‹ç›®å‰ vLLM çš„ä¼˜åŠ¿ï¼ŒåŠå…¶ç‰¹æ•ˆå¯¹åº”çš„å®ç°æˆ–è€… Exampleã€‚

- Distributed Inference and Servingï¼Œæ”¯æŒä½¿ç”¨  [Megatron-LMâ€™s tensor parallel algorithm](https://arxiv.org/pdf/1909.08053.pdf) çš„å¤šçº§å¤šå¡çš„åˆ†å¸ƒå¼ inferenceã€‚å…¶ä¸­ï¼š
  - å¤šå¡åªéœ€è¦è®¾ç½® `llm = LLM("facebook/opt-13b", tensor_parallel_size=4)` 
  - å¤šæœºéœ€è¦å’Œ [Ray runtime](https://docs.ray.io/en/latest/ray-core/starting-ray.html) ä¸€èµ·ä½¿ç”¨ã€‚ã€ç•¥å¤æ‚ã€‘
- Running on clouds with SkyPilot, vLLM æ”¯æŒé€šè¿‡ SkyPilot åœ¨ä»»ä½•äº‘ä¸Šç›´æ¥ä½¿ç”¨ã€‚
  - é¦–å…ˆï¼Œéœ€è¦äº†è§£çš„æ˜¯SkyPilot æ˜¯ä¸€ä¸ªå¼€æºçš„æ¡†æ¶ï¼Œç”¨äºåœ¨ä»»ä½•äº‘ç¯å¢ƒä¸Šæ— ç¼ã€ä¸”ç»æµé«˜æ•ˆåœ°è¿è¡Œæœºå™¨å­¦ä¹ ä¸LLMä»»åŠ¡ã€‚è¿™é‡Œæœ‰ä¸€ç¯‡å¾ˆä¸é”™çš„[ä»‹ç»](https://zhuanlan.zhihu.com/p/591958927)ã€‚
  - è¿™é‡Œä¹Ÿå·²ç»æœ‰ä¸€ç¯‡å®Œæ•´çš„åšå®¢ [ã€ŠSky Pilotï¼šä¸€é”®åœ¨ä»»æ„äº‘ä¸Šè¿è¡Œ LLMsã€‹](https://guoxudong.io/post/skypilot/)ï¼Œç”¨äºä»‹ç»å¦‚ä½•é€šè¿‡ SkyPilot åœ¨ **Azure** ä¸Šéƒ¨ç½² Llama-2 Chatbotã€‚
- Deploying with NVIDIA Tritonï¼Œ[Triton Inference Server](https://github.com/triton-inference-server) æä¾›äº†ä¸€ä¸ªä¸“é—¨çš„æ–‡æ¡£ç”¨äºæŒ‡å¯¼**å¦‚ä½•éƒ¨ç½²ä¸€ä¸ªä½¿ç”¨äº† vLLM çš„æ¨¡å‹**ï¼š[Deploying a vLLM model in Triton](https://github.com/triton-inference-server/tutorials/blob/main/Quick_Deploy/vLLM/README.md#deploying-a-vllm-model-in-triton)ã€‚
- Deploying with Dockerï¼ŒvLLM æä¾›äº†å®˜æ–¹çš„ docker image ç”¨æ¥éƒ¨ç½²ã€‚
- Production Metricsï¼ŒvLLM å…¬å¼€äº†è®¸å¤šå¯ç”¨äºç›‘æ§ç³»ç»Ÿå¥åº·çŠ¶å†µçš„æŒ‡æ ‡ã€‚ä¾‹å¦‚ï¼š`gauge_avg_generation_throughput`ã€‚å¹¶ä¸”**è¿˜æä¾›äº†å®Œæ•´çš„ [benchmark è„šæœ¬ç›´](https://github.com/vllm-project/vllm/tree/main/benchmarks)æ¥ä½¿ç”¨ã€‚**

### Model

vLLM ç›´æ¥æ”¯æŒ hugging face ä¸­çš„ä¸€äº›æ¨¡å‹ç»“æ„ã€‚å½“ç„¶ï¼Œæœ€ç®€å•çš„æ–¹æ³•ä¾¿æ˜¯é€šè¿‡ä»¥ä¸‹çš„æµç¨‹å»å®é™…æµ‹è¯•æ¨¡å‹èƒ½å¦æ­£å¸¸çš„ç”Ÿæˆ outputã€‚

> The easiest way to check if your model is supported is to run the program below:
>
> ```
> from vllm import LLM
> 
> llm = LLM(model=...)  # Name or path of your model
> output = llm.generate("Hello, my name is")
> print(output)
> ```
>
> If vLLM successfully generates text, it indicates that your model is supported.

#### Adding a new model.

ä½†æ˜¯ï¼Œ**å½“ vLLM ä¸æ”¯æŒæˆ‘ä»¬çš„æ¨¡å‹æ—¶**ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡ä¸€ç³»åˆ—æ­¥éª¤æ³¨å†Œæˆ‘ä»¬çš„æ¨¡å‹ã€‚vLLM æä¾›äº†ä¸€ä»½è¯¦ç»†çš„å®˜æ–¹æŒ‡å— [vLLM: Adding a New Model](https://docs.vllm.ai/en/latest/models/adding_model.html)ã€‚å¯¹äºæ·»åŠ æ–°æ¨¡å‹çš„å¤æ‚æ€§ï¼ŒvLLM å›¢é˜Ÿä¹Ÿç»™å‡ºäº†ä¸€äº›æ³¨æ„äº‹é¡¹ï¼š

> The complexity of adding a new model depends heavily on the modelâ€™s architecture. The process is considerably straightforward if the model shares a similar architecture with an existing model in vLLM. However, for models that include new operators (e.g., a new attention mechanism), the process can be a bit more complex.
>
> æ¨¡å‹ç»“æ„è¶Šæ¥è¿‘ vLLM å·²æ”¯æŒçš„æ¨¡å‹ï¼Œåˆ™æ³¨å†Œæ›´ä¸ºç®€å•ã€‚

### Quantization

ç›®å‰ vLLM æ”¯æŒçš„æ˜¯ AWQ ï¼ˆActivation-aware Weight Quantizationï¼‰é‡åŒ–ã€‚ä½†æ˜¯ vLLM å®˜æ–¹è¿›è¡Œäº†æé†’ï¼š

> Please note that AWQ support in vLLM is **under-optimized** at the moment. We would recommend using the **unquantized version of the model** for better accuracy and higher throughput. 

ä¹Ÿå°±æ˜¯ vLLM ç›®å‰==ä¸æ¨èä½¿ç”¨é‡åŒ–åçš„æ¨¡å‹==ã€‚å¦‚æœç¡®å®éœ€è¦ï¼Œåˆ™ä¸»è¦å€ŸåŠ©äº [AutoAWQ](https://github.com/casper-hansen/AutoAWQ) æ¡†æ¶ã€‚æ•´ä½“çš„é‡åŒ–æµç¨‹ä¹Ÿå¾ˆç®€å•ï¼Œdoc ä¸­æä¾›äº†ä¸€ä¸ªæ¸…æ™°çš„ [Example](https://docs.vllm.ai/en/latest/quantization/auto_awq.html)ã€‚

### å¯èƒ½çš„ç¼ºé™·

- vLLM çš„æ¨ç†ç»“æœå’Œ hf çš„ä¸ä¸€è‡´ã€‚
- `batch_size = 1` çš„æ—¶å€™ï¼Œæ¨ç†é€Ÿåº¦çš„ä¼˜åŒ–åŠ›åº¦ä¸æ˜æ˜¾ã€‚

### Comparison with `TensorRT-LLM`

ç›®å‰å¹¶æ²¡æœ‰å®˜æ–¹å…¬å¼€çš„å¯¹æ¯”æ•°æ®ã€‚ä¾æ® Github ä¸Š NVIDIA å·¥ä½œäººå‘˜çš„å‘è¨€ï¼š

> We do not plan to publish performance numbers that compare TensorRT-LLM with vLLM.
>
> Our internal measurements show that TensorRT-LLMâ€™s in-flight batching and paged KV cache features work well and TensorRT-LLM can deliver great performance. Weâ€™d be happy to provide you with performance numbers for relevant cases.
>
> Is there a particular use case that youâ€™d be interested in?

æˆ‘è®¤ä¸º TensorRT-LLM å¯¹æ¯” vLLM **åº”è¯¥æ²¡æœ‰å·¨å¤§çš„æ€§èƒ½æå‡**ã€‚æ›´ä¸ºå¯é çš„è¯æ®æºè‡ªæŸä¸ªæ¨ç‰¹ [[â€]](https://twitter.com/HamelHusain/status/1719872352694174093) ï¼Œè¯¥æ¨ç‰¹å†…å®¹è¯´æ˜åœ¨åŒæ ·ä½¿ç”¨ triton server è¿›è¡Œ Llama çš„ inference æƒ…å†µä¸‹ï¼ŒvLLM çš„æ€§èƒ½æ¯” TensorRT æ›´ä¼˜ã€‚

## vLLM çš„å®ç°åŸç†/Code

- [vLLMæ¡†æ¶åŸç†â€”â€”PagedAttention](https://zhuanlan.zhihu.com/p/649537608)

> Q: PageAttention ä¸­ï¼š***åºåˆ—åœ¨åˆ†å—ä¹‹åï¼Œåªæœ‰æœ€åä¸€ä¸ªå—å¯èƒ½ä¼šæµªè´¹å†…å­˜ï¼ˆå®é™…ä¸­æµªè´¹çš„å†…å­˜ä½äº4%ï¼‰***
>
> ä¸ºä»€ä¹ˆåªæœ‰æœ€åä¸€ä¸ªå—å¯èƒ½ä¼šæµªè´¹å†…å­˜å‘¢ï¼Ÿèƒ½ä¸ºæˆ‘è§£é‡Šä¸‹å—ï¼Ÿ

è¿™é‡Œçš„å—æ˜¯ `block` çš„æ„æ€ï¼Œä¹Ÿå°±æ˜¯è¿™ä¸ª block ä¸­æœ‰å¾ˆå¤š slotï¼Œæ‰€ä»¥è¿™é‡Œçš„æ„æ€æ˜¯æœ€åä¸€ä¸ª `block` å¯èƒ½å¡«ä¸æ»¡ã€‚

> æ²¡æœ‰ PageAttention ä¹‹å‰ä¸ºä»€ä¹ˆä¼šæœ‰å†…å­˜æµªè´¹å‘¢ï¼Ÿæ²¡æœ‰ PageAttention ä¹‹å‰çš„ KV Cache æ˜¯æ€ä¹ˆç®¡ç†çš„å‘¢ï¼Ÿ

æŒ‰ç…§ç‰©ç†è¿ç»­å­˜å‚¨çš„æ–¹å¼å¯èƒ½ä¼šé€ æˆæ˜¾å­˜ç¢ç‰‡åŒ–ã€‚æ™®é€šçš„huggingfaceçš„kv cacheæ˜¯æ¯ä¸ªç”¨æˆ·ç»™ä¸€ä¸ªå›ºå®šå¤§å°çš„æ˜¾å­˜ç©ºé—´ï¼Œæœ‰æµªè´¹ï¼Œè¿™ä¸ªç›¸å½“äºåˆ‡æˆå‡ ä¸ªå­—å‡ ä¸ªå­—æ‰€æœ‰ç”¨æˆ·å¡åœ¨ä¸€å—ï¼Œæœ‰å¤šå°‘ç”¨å¤šå°‘ï¼Œä¸ä¼šæµªè´¹ã€‚

- [LLM é«˜é€Ÿæ¨ç†æ¡†æ¶ vLLM æºä»£ç åˆ†æ / vLLM Source Code Analysis](https://zhuanlan.zhihu.com/p/641999400)
- [vLLMçš‡å† ä¸Šçš„æ˜ç ï¼šæ·±å…¥æµ…å‡ºç†è§£PagedAttention CUDAå®ç°](https://zhuanlan.zhihu.com/p/673284781)
- [vLLM & PagedAttention è®ºæ–‡æ·±åº¦è§£è¯»ï¼ˆä¸€ï¼‰â€”â€” LLM æœåŠ¡ç°çŠ¶ä¸ä¼˜åŒ–æ€è·¯](https://zhuanlan.zhihu.com/p/656939628)

- [å¤§æ¨¡å‹æ¨ç†æ€§èƒ½ä¼˜åŒ–ä¹‹KV Cacheè§£è¯»](https://zhuanlan.zhihu.com/p/630832593)

- [å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿï¼šçœ‹å›¾å­¦KV Cache](https://zhuanlan.zhihu.com/p/662498827)

> è¿™ç¯‡è¿˜ä¸é”™ï¼Œé€šè¿‡å¯è§†åŒ–ä¸€æ­¥æ­¥çš„è®¡ç®—æ­¥éª¤å±•ç¤ºäº† KV Cache çš„ä¼˜åŒ–ã€‚ï¼ˆKV Cache æ˜¯åŸºç¡€ä¼˜åŒ–ï¼‰åŒæ—¶è¿˜è¯´æ˜äº† KV Cache æ˜¯å†…å­˜åˆºå®¢çš„é—®é¢˜ã€‚
>

- [ææ™ºAI | å¤§æ¨¡å‹ä¼˜åŒ–ä¹‹KV Cache](https://juejin.cn/post/7287768247889559611)
