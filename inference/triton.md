# NVIDIA Triton Inference Server

*Deploy, run, and scale AI for any application on any platform.*

ä¸»è¦æœ‰ä¸¤ä¸ª topic éœ€è¦æ¢ç´¢ï¼š

- Triton Inference Server + Client
- PyTriton

æˆ‘ä»¬éœ€è¦åˆ†åˆ«ææ¸…æ¥šä»–ä»¬æ˜¯ä»€ä¹ˆï¼Ÿå¯¹ PyTriton æœ‰å…´è¶£çš„å¯ä»¥ç›´æ¥è·³è½¬åˆ° PyTriton ç« èŠ‚ã€‚

>  NVIDIA Tritonâ„¢ æ¨ç†æœåŠ¡å™¨æ˜¯ NVIDIA AI å¹³å°çš„ä¸€éƒ¨åˆ†ï¼Œå®ƒæ˜¯ä¸€æ¬¾å¼€æºæ¨ç†æœåŠ¡è½¯ä»¶ï¼Œå¯åŠ©åŠ›æ ‡å‡†åŒ–æ¨¡å‹çš„éƒ¨ç½²å’Œæ‰§è¡Œï¼Œå¹¶åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æä¾›å¿«é€Ÿä¸”å¯æ‰©å±•çš„ AIã€‚

## Introduction

### What's NVIDIA Triton?

é¦–å…ˆä¸€ä¸ªä¸»è¦é—®é¢˜æ˜¯ï¼šä»€ä¹ˆæ˜¯ Tritonï¼Ÿ

> Triton å°±æ˜¯ä¸€ä¸ªæ¨ç†æœåŠ¡å™¨ï¼Œæ—¨åœ¨å®ç°ç»Ÿä¸€çš„ AI æ¨ç†ã€‚ä¹Ÿå°±æ˜¯æ»¡è¶³â€œä»»ä½•â€AIæ¨ç†éœ€æ±‚ã€‚
>
> è¿™æ˜¯æ›´è¯¦ç»†çš„ï¼šTriton æ¨ç†æœåŠ¡å™¨å¯åŠ©åŠ›å›¢é˜Ÿåœ¨ä»»æ„åŸºäº GPU æˆ– CPU çš„åŸºç¡€è®¾æ–½ä¸Šéƒ¨ç½²ã€è¿è¡Œå’Œæ‰©å±•ä»»æ„æ¡†æ¶ä¸­ç»è¿‡è®­ç»ƒçš„ AI æ¨¡å‹ï¼Œè¿›è€Œç²¾ç®€ AI æ¨ç†ã€‚åŒæ—¶ï¼ŒAI ç ”ç©¶äººå‘˜å’Œæ•°æ®ç§‘å­¦å®¶å¯åœ¨ä¸å½±å“ç”Ÿäº§éƒ¨ç½²çš„æƒ…å†µä¸‹ï¼Œé’ˆå¯¹å…¶é¡¹ç›®è‡ªç”±é€‰æ‹©åˆé€‚çš„æ¡†æ¶ã€‚å®ƒè¿˜å¸®åŠ©å¼€å‘è€…è·¨äº‘ã€æœ¬åœ°ã€è¾¹ç¼˜å’ŒåµŒå…¥å¼è®¾å¤‡æä¾›é«˜æ€§èƒ½æ¨ç†ã€‚

### Triton Inference Server Features

è¿™ä¸ªé—®é¢˜ç­‰ä»·äºï¼š **Why NVIDIA Tritonï¼Ÿ**

![triton-adventages](./assets/triton-adventages.png)

Triton çš„ä¸»è¦ä¼˜åŠ¿å¦‚ä¸Šå›¾ï¼Œ**å¦‚æœå¯¹äºè¿™éƒ¨åˆ†ä¸æ„Ÿå…´è¶£å¯ä»¥å¿«é€Ÿçš„è·³åˆ°åç»­çš„ç« èŠ‚**ã€‚æ¥ä¸‹æ¥æˆ‘éœ€è¦ææ¸…æ¥š Triton çš„ä¸»è¦ä¼˜åŠ¿ï¼Œä¸ºä»€ä¹ˆ Triton Server ä¼šè¢«è¿™ä¹ˆå¤šä¸“ä¸šçš„äº§å“ï¼Œå›¢é˜Ÿæ‰€ä½¿ç”¨ã€‚

- **ğŸ”¥ æ”¯æŒå¤šä¸ªæ¡†æ¶ï¼ˆSupports All Training and Inference Frameworksï¼‰**
  - Triton æ¨ç†æœåŠ¡å™¨æ”¯æŒæ‰€æœ‰ä¸»æµæ¡†æ¶ï¼Œä¾‹å¦‚ TensorFlowã€NVIDIAÂ® TensorRTâ„¢ã€PyTorchã€MXNetã€Pythonã€ONNXã€RAPIDSâ„¢ FILï¼ˆç”¨äº XGBoostã€scikit-learn ç­‰ï¼‰ã€OpenVINOã€è‡ªå®šä¹‰ C++ ç­‰ã€‚
  - **é‡è¦æ€§ï¼š**è¿™ä¸ªæ¯«æ— ç–‘é—®æ˜¯æœ€é‡è¦ï¼Œä¹Ÿæ˜¯ Triton æœ€å—æ¬¢è¿çš„ç‰¹æ•ˆä¹‹ä¸€ï¼Œæ”¯æŒå¤§é‡çš„æ¡†æ¶ã€‚
- **ğŸš€ é«˜æ€§èƒ½æ¨ç†ï¼ˆHigh-Performance Inferenceï¼‰**
  - ä¸­æ–‡ï¼šTriton æ”¯æŒæ‰€æœ‰åŸºäº NVIDIA GPUã€x86 å’Œ ARMÂ® CPU çš„æ¨ç†ã€‚å®ƒå…·æœ‰**åŠ¨æ€æ‰¹å¤„ç†**ã€**å¹¶å‘æ‰§è¡Œ**ã€**æœ€ä¼˜æ¨¡å‹é…ç½®**ã€**æ¨¡å‹é›†æˆ**å’Œ**ä¸²æµè¾“å…¥**ç­‰åŠŸèƒ½ï¼Œå¯æ›´å¤§é™åº¦åœ°æé«˜ååé‡å’Œåˆ©ç”¨ç‡ã€‚
  - English: Maximize throughput and utilization with **dynamic batching**, **concurrent execution**, **optimal configuration**, and **streaming audio and video**. Triton Inference Server supports all NVIDIA GPUs, x86 and ArmÂ® CPUs, and AWS Inferentia.
  - **é‡è¦æ€§ï¼š** æ¨ç†æœåŠ¡å™¨ï¼Œæœ€é‡è¦çš„å°±æ˜¯æ¨ç†æ€§èƒ½ï¼ŒNVIDAI ä½œä¸º AI è®¡ç®—çš„è¡Œä¸šé¾™å¤´ Triton åœ¨æ€§èƒ½è¿™å—ä¼˜åŠ¿è‚¯å®šéå¸¸æ˜¾è‘—ã€‚
- ä¸“ä¸º DevOps å’Œ MLOps è®¾è®¡ï¼ˆè¿™æ®µæš‚æ—¶å¿½ç•¥ ï¼Œæ„Ÿè§‰å’Œäº‘è®¡ç®—é‚£ä¸€å¥—æ¯”è¾ƒæ²¾è¾¹ï¼‰
  - Triton ä¸ Kubernetes é›†æˆï¼Œå¯ç”¨äºç¼–æ’å’Œæ‰©å±•ï¼Œå¯¼å‡º Prometheus æŒ‡æ ‡è¿›è¡Œç›‘æ§ï¼Œæ”¯æŒå®æ—¶æ¨¡å‹æ›´æ–°ï¼Œå¹¶å¯ç”¨äºæ‰€æœ‰ä¸»æµçš„å…¬æœ‰äº‘ AI å’Œ Kubernetes å¹³å°ã€‚å®ƒè¿˜ä¸è®¸å¤š MLOPS è½¯ä»¶è§£å†³æ–¹æ¡ˆé›†æˆã€‚

> å¯¹äºç”¨æˆ·æ¥è¯´ï¼Œæˆ‘ä»¬å¯èƒ½æ›´å…³ç³»çš„æ˜¯é«˜æ€§èƒ½æ¨ç†éƒ¨åˆ†æ‰€ä½¿ç”¨çš„æŠ€æœ¯ç»†èŠ‚ï¼ˆæˆ‘å°±æ˜¯è¿™æ ·ï¼‰ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç©¶ã€‚æˆ‘ä»¬å°†ä»‹ç»ï¼š
>
> - [x] Dynamic Batching
> - [x] Concurrent Execution
> - [ ] Optimal Configuration
> - [ ] Model Ensemble
> - [ ] Streaming audio and Vedio
>
> ### Dynamic Batching
>
> Batchå¯¹äºGPUä¸Šæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è¿è¡Œæ•ˆç‡å½±å“å¾ˆå¤§ã€‚åœ¨ Inference æ—¶ï¼Œå¦‚æœåŒæ—¶é¢ä¸´å¤šä¸ªè¯·æ±‚ï¼Œå¦‚æœæƒ³æå‡ååé‡ï¼Œéœ€è¦æˆ‘ä»¬æŠŠè¿™å¤šä¸ªè¯·æ±‚ç»„åˆæˆä¸€ä¸ª batch ç”± GPU è¿›è¡Œ Batch Inferenceã€‚æ‰€ä»¥ Triton ä¸­çš„ Dynamic Batching ä¾¿æ˜¯å®ç°äº†è¿™æ ·çš„åŠŸèƒ½ã€‚
>
> Triton æ”¯æŒè®¾ç½® `max batch size` å¤§å°å’Œ `delay` åŒºé—´ã€‚æˆ‘ä»¬é€šè¿‡ä¸‹å›¾å¯ä»¥å‘ç°å¼€å¯ Dynamic Batching å¹¶è®¾ç½®åˆç†çš„ Delay å¯ä»¥å¤§å¹…å¢å¼ºååé‡ã€‚
>
> <img src="./assets/dynamic_batching.png" alt="img" style="zoom: 50%;" />
>
> ### Concurrent Execution
>
> Concurrent Execution åœ¨æˆ‘çœ‹æ¥å¯ä»¥ç«‹å³ä¸º**è‡ªåŠ¨ç®¡ç†å¤šçº¿ç¨‹æ‰§è¡Œæ¨¡å‹æ¨ç†ä»»åŠ¡ã€‚**
>
> å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬åœ¨ GPU ä¸Šå¼€äº† 4 ä¸ªå®ä¾‹ï¼Œå…¶ä¸­ 1 ä¸ªç”¨æ¥è¿›è¡Œ Model0 çš„æ¨ç†ï¼Œ3 ä¸ªç”¨æ¥è¿›è¡Œ Model1 çš„æ¨ç†ã€‚
>
> ![concurrent-execution](./assets/concurrent-execution.png)
>
> ä¸‹å›¾ä¾¿æ˜¯ Triton åŒæ—¶ä½¿ç”¨ Dynamic Batching å’Œ Concurrent Executionã€‚ä¸Šä¸‹å¯¹æ¯”æˆ‘ä»¬å‘ç°æ€§èƒ½è¿›ä¸€æ­¥æå‡ã€‚
>
> <img src="./assets/dynamic-batch-and-concurrent-execution.png" alt="dynamic-batch-and-concurrent-execution" style="zoom:50%;" />
>
> å‰©ä½™æ²¡æœ‰æ¢ç´¢çš„å†…å®¹å¯ä»¥å‚è€ƒå¼•ç”¨å¦‚ä¸‹ï¼š
>
> - [çŸ¥ä¹ï¼šDynamic Batchingï¼](https://zhuanlan.zhihu.com/p/354633729)
> - [ğŸ”¥ triton-inference-serverçš„backendï¼ˆä¸€ï¼‰â€”â€”å…³äºæ¨ç†æ¡†æ¶çš„ä¸€äº›è®¨è®º](https://zhuanlan.zhihu.com/p/666655108)
> - [Model Ensemble: Serving ML Model Pipelines on NVIDIA Triton Inference Server with Ensemble Models](https://developer.nvidia.com/blog/serving-ml-model-pipelines-on-nvidia-triton-inference-server-with-ensemble-models/)
> - [NVIDIA: Concurrent inference and dynamic batching](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/examples/jetson/concurrency_and_dynamic_batching/README.html)
> - [**æœ€ä¼˜æ¨¡å‹é…ç½®**: NVIDIA Triton Model Analyzer](https://github.com/triton-inference-server/model_analyzer)

## Install

å¦‚ä½•è¿›è¡Œå®‰è£…ã€‚

## Example å®æˆ˜ Llama-7b



# ğŸ”¥PyTriton

## Installation

ä¸»è¦å†…å®¹å¯ä»¥å‚è€ƒ NVIDAI çš„å®˜æ–¹æ–‡æ¡£ï¼š[Offical Installation](https://triton-inference-server.github.io/pytriton/latest/installation/)ã€‚ä¾æ®å›½å†…çš„ä¹ æƒ¯ï¼Œæˆ‘æ¨èå‚è€ƒ Creating virtualenv using `miniconda` è¿™ä¸ª partã€‚æˆ‘è‡ªå·±çš„å®‰è£…ç»éªŒæ˜¯ï¼š

```bash
# Create your virtual environment.
...

# Export the conda library path
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib

pip install -U nvidia-pytriton
```

ï¼ˆä¸‹é¢çš„å†…å®¹å¯ä»¥è·³è¿‡ï¼‰ä¹‹æ‰€ä»¥éœ€è¦è¿™ä¸ª `export` è¯­å¥æ˜¯å› ä¸ºåœ¨å®‰è£…æ—¶é‡åˆ°äº†ä¸€ä¸ªå° BUGï¼Œåœ¨å®‰è£…åè¿è¡Œäº†ä¸€æ®µ Example ä»£ç ä½†æ˜¯é‡åˆ°æŠ¥é”™ä¿¡æ¯ï¼š

```
I0212 16:32:55.775014 9771 model_lifecycle.cc:461] loading: Linear:1
/root/.cache/pytriton/workspace_qgciftzs/tritonserver/backends/python/triton_python_backend_stub: error while loading shared libraries: libpython3.9.so.1.0: cannot open shared object file: No such file or directory
```

å¤§æ¦‚æ„æ€æ˜¯ `libpython3.9.so.1.0` è¿™ä¹ˆä¸€ä¸ªåŠ¨æ€é“¾æ¥åº“æ²¡æ‰¾åˆ°ã€‚å…¶å® Official Installation é‡Œé¢ä¹Ÿæœ‰å…³äºè¿™éƒ¨åˆ†çš„è¯´æ˜ã€‚æˆ‘ä»¬åªéœ€è¦åœ¨å½“å‰çš„ SHELL ä¸­æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ã€‚

```bash
# Export the conda library path
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib
```

> å…¶ä¸­ `CONDA_PREFIX` æ˜¯ä¸€ä¸ªç¯å¢ƒå˜é‡ï¼Œå®ƒè¡¨ç¤ºå½“å‰æ¿€æ´»çš„condaç¯å¢ƒçš„è·¯å¾„ã€‚å½“ä½ ä½¿ç”¨condaæ¿€æ´»ä¸€ä¸ªç‰¹å®šçš„ç¯å¢ƒæ—¶ï¼Œcondaä¼šè‡ªåŠ¨è®¾ç½®`CONDA_PREFIX`å˜é‡æ¥æŒ‡å‘é‚£ä¸ªç¯å¢ƒçš„æ ¹ç›®å½•ã€‚è€Œå…³äº **LD_LIBRARY_PATH** æ˜¯ä»€ä¹ˆå¯ä»¥å‚è€ƒæˆ‘çš„ä»“åº“ä¸­çš„ [`env/cuda-related.md`](https://github.com/keli-wen/AGI-Study/blob/master/env/cuda-related.md#3-multi-cuda-management)ã€‚

**âš ï¸æ³¨æ„æˆ‘ä»¬ä¸èƒ½ç›´æ¥åœ¨ `~/.bashrc` æˆ–è€… `~/.zshrc` ä¸­æ·»åŠ ä¸Šè¿°å‘½ä»¤**ã€‚å› ä¸ºæˆ‘ä»¬éœ€è¦ source ä¿è¯æ›´æ–°è¢«åº”ç”¨ï¼Œä½†æ˜¯**é€šå¸¸æƒ…å†µ**ä¸‹ source ä¼šæ”¹å˜å½“å‰ conda è™šæ‹Ÿç¯å¢ƒï¼Œä½¿å¾— `CONDA_PREFIX` æ’å®šä¸º `/opt/conda`ï¼ˆä¹Ÿå°±æ˜¯ base ç¯å¢ƒï¼‰ã€‚

## Introduction

> ä¸€ä¸ªåˆšå‘å¸ƒä¸ä¹…çš„æ¡†æ¶ã€‚å®ƒå…¶å®å°±æ˜¯è®©æˆ‘ä»¬å¯ä»¥åœ¨ Python ç«¯ç›´æ¥å¯åŠ¨ Triton Inference Serverã€‚ 

â€œPyTriton is a Flask/FastAPI-like interface that simplifies Triton's deployment in Python environments. The library allows serving Machine Learning models directly from Python through NVIDIA's [Triton Inference Server](https://github.com/triton-inference-server). The solution is framework-agnostic and can be used along with frameworks like PyTorch, TensorFlow, or JAX.â€

PyTriton æä¾›ä¸€ç§é€‰æ‹©è®©ä½ çš„ Python æ¨¡å‹å¯ä»¥ä½¿ç”¨ Triton Inference Server æ¥å¤„ç† HTTP/gRPC è¯·æ±‚ã€‚æˆ‘ä»¬ä½¿ç”¨**é˜»å¡æ¨¡å¼**ï¼Œä¹Ÿå°±æ˜¯è¿™ä¸ªç¨‹åºä¼šé•¿æœŸè¿è¡Œåœ¨ä½ éƒ¨ç½²çš„é›†ç¾¤ä¸Šæ¥å¤„ç†æ¥è‡ª client çš„å„ç§è¯·æ±‚ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬è¦å®šä¹‰å¥½ä¸€ä¸ª `infer_fn` å›è°ƒå‡½æ•°ç”¨æ¥å¤„ç† inferenceã€‚å¦‚ä¸‹ï¼Œè¿™é‡Œçš„ä¿®é¥°å™¨ `@batch` ä»£è¡¨è¯¥æ¨¡å‹æ¥å— batch è¾“å…¥ã€‚

```py
import numpy as np
from pytriton.decorators import batch


@batch
def infer_fn(**inputs: np.ndarray):
    input1, input2 = inputs.values()
    outputs = model(input1, input2)
    return [outputs]
```

ä¸‹ä¸€æ­¥åˆ™æ˜¯é€šè¿‡ä»£ç å°† Triton å’Œ `infer_fn` è¿æ¥ã€‚åªéœ€è¦ `bind` å‡½æ•°ä¾¿å¯ä»¥å®ç°ã€‚

åœ¨é˜»å¡æ¨¡å¼ä¸‹ï¼Œå»ºè®®ä½¿ç”¨ Triton å¯¹è±¡ä½œä¸ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚ï¼ˆä¸ºä»€ä¹ˆæˆ‘ä¹Ÿæ²¡ç†è§£ï¼‰

é€šè¿‡å¦‚ä¸‹ä»£ç ä¾¿å·²ç»å®šä¹‰å¥½äº† Triton è¯¥å¦‚ä½•å¤„ç†æ¨¡å‹ä»¥åŠ HTTP/gRPC è¯·æ±‚åº”è¢«å®šå‘åˆ°ä½•å¤„ã€‚

```python
with Triton() as triton:
    triton.bind(
        model_name="MyModel",
        infer_func=infer_fn,
        inputs=[
            Tensor(dtype=bytes, shape=(1,)),  # sample containing single bytes value
            Tensor(dtype=bytes, shape=(-1,)),  # sample containing vector of bytes
        ],
        outputs=[
            Tensor(dtype=np.float32, shape=(-1,)),
        ],
        config=ModelConfig(max_batch_size=16),
    )
    ...
    triton.serve()
```

æœ€åä¸€æ­¥ä¹‹éœ€è¦ä½¿ç”¨ `triton.serve()` ä¾¿å¯åŠ¨äº† Triton Inference Serveï¼Œæ‰€æœ‰çš„è¯·æ±‚ä¼šè¢«é‡å®šå‘åˆ° `localhost:8000/v2/models/MyModel/infer` å¹¶ç”± `infer_fn` æ‰§è¡Œæ¨ç†ä»»åŠ¡ã€‚

## Quick Start

å¦‚æœä½ æƒ³è¦å¿«é€Ÿäº†è§£ PyTriton çš„ä½¿ç”¨ï¼Œé˜…è¯» [Official: Quick Start](https://triton-inference-server.github.io/pytriton/latest/quick_start/) æ˜¯æœ€ç›´æ¥ã€‚**å®ƒåˆ†åˆ«ä»‹ç»äº† `server` å’Œ `client` çš„å®šä¹‰ä¸ä½¿ç”¨ã€‚**

å¯¹äº `server`ï¼Œä¸»è¦æ˜¯å¦‚ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

1. å®šä¹‰ `model`ï¼Œè¿™é‡Œæ˜¯ `Linear`ã€‚
2. å®šä¹‰ `infer_fn` ç”¨äºåç»­çš„ç»‘å®šæ“ä½œã€‚
3. é€šè¿‡ `triton.bind(*)` å’Œ `torch.serve()` ã€‚

```python
import numpy as np
import torch

from pytriton.decorators import batch
from pytriton.model_config import ModelConfig, Tensor
from pytriton.triton import Triton

model = torch.nn.Linear(2, 3).to("cuda").eval()


@batch
def infer_fn(**inputs: np.ndarray):
    (input1_batch,) = inputs.values()
    input1_batch_tensor = torch.from_numpy(input1_batch).to("cuda")
    output1_batch_tensor = model(
        input1_batch_tensor
    )  # Calling the Python model inference
    output1_batch = output1_batch_tensor.cpu().detach().numpy()
    return [output1_batch]


def main():
    # Connecting inference callable with Triton Inference Server
    with Triton() as triton:
        triton.bind(
            model_name="Linear",
            infer_func=infer_fn,
            inputs=[
                Tensor(dtype=np.float32, shape=(-1,)),
            ],
            outputs=[
                Tensor(dtype=np.float32, shape=(-1,)),
            ],
            config=ModelConfig(max_batch_size=128),
        )
        triton.serve()

if __name__ == "__main__":
    main()
```

å¯¹äº `client`ï¼Œå¯ä»¥é€šè¿‡ `curl` è¿›è¡Œè®¿é—®ï¼ˆå…·ä½“å¯ä»¥å‚è€ƒä¸Šé¢çš„é“¾æ¥ï¼‰ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬æ›´ç†Ÿæ‚‰çš„æ˜¯ä½¿ç”¨ Pythonã€‚

```python
import torch
from pytriton.client import ModelClient

input1_data = torch.randn(128, 2).cpu().detach().numpy()

with ModelClient("localhost:8000", "Linear") as client:
    result_dict = client.infer_batch(input1_data)

print(result_dict)
```

è¿™æ ·çœ‹æ¥ï¼ŒPyTriton ç¡®å®å¤§å¤§ç®€åŒ–äº† Triton Inference Server çš„ä½¿ç”¨æµç¨‹ã€‚

## Deep

### `TritonConfig`

TritonConfig ä¸»è¦æ˜¯å®šåˆ¶åŒ–æœåŠ¡å™¨çš„ä¸€äº›é…ç½®ã€‚åœ¨**é€šå¸¸æƒ…å†µä¸‹æˆ‘ä»¬å¯èƒ½ä¸éœ€è¦å¯¹æ­¤è¿›è¡Œè®¾ç½®**ï¼Œå¦‚æœéœ€è¦è®¾ç½®è¯·å‚è€ƒ [TritonConfig](https://triton-inference-server.github.io/pytriton/latest/reference/triton_config/)ã€‚

### Binding Models to Triton

PyTriton ä½¿ç”¨ `bind` æ–¹æ³•æ¥å‘Šè¯‰ Triton å¦‚ä½•å¤„ç†æ¨¡å‹æ¨ç†è¯·æ±‚ã€‚æ‰€ä»¥å®Œå…¨çš„äº†è§£ `bind` æ“ä½œå¯¹äºæˆ‘ä»¬ç†Ÿç»ƒä½¿ç”¨ PyTriton éå¸¸é‡è¦ã€‚è§è¿™ä¸ªç®€å•çš„ Exampleï¼š

```python
import numpy as np
from pytriton.decorators import batch
from pytriton.model_config import ModelConfig, Tensor
from pytriton.triton import Triton


@batch
def infer_fn(**inputs: np.ndarray):
    input1, input2 = inputs.values()
    outputs = model(input1, input2)
    return [outputs]

with Triton() as triton:
  triton.bind(
      model_name="ModelName",
      infer_func=infer_fn,
      inputs=[
          Tensor(shape=(1,), dtype=np.bytes_),  # sample containing single bytes value
          Tensor(shape=(-1,), dtype=np.bytes_)  # sample containing vector of bytes
      ],
      outputs=[
          Tensor(shape=(-1,), dtype=np.float32),
      ],
      config=ModelConfig(max_batch_size=8),
      strict=True,
  )
```

æˆ‘ä»¬å‘ç° `triton.bind()` åŒ…å«å¦‚ä¸‹å‚æ•°ï¼š

- `model_name`ï¼šä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä»£è¡¨ Triton Inference Server ä¸­å¯ç”¨çš„ Model åï¼Œåœ¨å‘é€è¯·æ±‚æ—¶æœ‰ç”¨ã€‚
- `infer_func`ï¼šPython å‡½æ•°ï¼Œä»£è¡¨ Triton Inference Server è¯¥å¦‚ä½•å¤„ç†æ¨ç†è¯·æ±‚ã€‚
- `inputs`ï¼šå®šä¹‰æ¨¡å‹è¾“å…¥å‚æ•°çš„æ•°é‡ï¼Œç±»å‹å’Œå½¢çŠ¶ã€‚
- `outputs`ï¼šå®šä¹‰æ¨¡å‹è¾“å‡ºå‚æ•°çš„æ•°é‡ï¼Œç±»å‹å’Œå½¢çŠ¶ã€‚
- `config`ï¼šç”¨äºå¯¹ Triton Inference Server ä¸Šçš„æ¨¡å‹éƒ¨ç½²è¿›è¡Œæ›´å¤šå®šåˆ¶ï¼Œæ¯”å¦‚ `batching` å’Œ `max_batch_size` ç­‰ã€‚
- `strict`ï¼šå¯ç”¨å¯¹**æ¨ç†è¾“å‡º**çš„æ•°æ®ç±»å‹å’Œå½¢çŠ¶è¿›è¡ŒéªŒè¯ï¼Œä»¥ç¡®ä¿å…¶ç¬¦åˆæä¾›çš„æ¨¡å‹é…ç½®ï¼ˆé»˜è®¤ä¸ºï¼š`False`ï¼‰ã€‚

### Inference Callable

Inference Callable ä¸»è¦æ˜¯ä¸ `infer_fn` ç›¸å…³ã€‚æ­£å¸¸æƒ…å†µä¸‹ä¾¿æ˜¯ç®€å•å®šä¹‰ä¸€ä¸ªå‡½æ•°å¤„ç†è¾“å…¥è¾“å‡ºå³å¯ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬è€ƒè™‘ä¸€äº›æ›´ä¸ºå¤æ‚çš„åœºæ™¯ã€‚

**â“Qï¼šåœ¨å¤š Instance èƒŒæ™¯ä¸‹ï¼Œå¦‚ä½•å®šä¹‰ `infer_fn`å‘¢ï¼Ÿ**

å½“æˆ‘ä»¬å­˜åœ¨å¤šä¸ªä¸åŒåœºæ™¯ä¸‹çš„æ¨ç†ä»»åŠ¡ï¼Œæˆ‘ä»¬éœ€è¦æä¾›ä¸€ä¸ª Inference Callable çš„åˆ—è¡¨ã€‚ **åŒºåˆ«äºä½¿ç”¨ Python å‡½æ•°ï¼Œåœ¨å¤šä»»åŠ¡æƒ…å†µä¸‹æˆ‘ä»¬å€¾å‘äºä½¿ç”¨ Wrapper Classï¼Œè¿™æ˜¯å› ä¸ºç±»å¯ä»¥ä¿æŒçŠ¶æ€ï¼Œé¿å…å¼€é”€ã€‚** å¦‚ä¸‹ï¼Œå¯ä»¥å‚è€ƒä¸€æ®µ codeï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ª `_InferFuncWrapper` ç”¨æ¥åœ¨ä¸åŒ device ä¸‹è¿›è¡Œæ¨¡å‹æ¨ç†ä»»åŠ¡ã€‚

```python
import torch
from pytriton.decorators import batch


class _InferFuncWrapper:
    def __init__(self, model: torch.nn.Module, device: str):
        self._model = model
        self._device = device

    @batch
    def __call__(self, **inputs):
        (input1_batch,) = inputs.values()
        input1_batch_tensor = torch.from_numpy(input1_batch).to(self._device)
        output1_batch_tensor = self._model(input1_batch_tensor)
        output1_batch = output1_batch_tensor.cpu().detach().numpy()
        return [output1_batch]
```

ç„¶åå†ä½¿ç”¨ä¸€ä¸ªå·¥å‚å‡½æ•°æ¥åˆ›å»ºå„ç§éœ€è¦çš„ `_InferFuncWrapper`ã€‚

```python
import numpy as np
from pytriton.triton import Triton
from pytriton.model_config import ModelConfig, Tensor


def _infer_function_factory(devices):
    infer_fns = []
    for device in devices:
        model = torch.nn.Linear(20, 30).to(device).eval()
        infer_fns.append(_InferFuncWrapper(model=model, device=device))

    return infer_fns


with Triton() as triton:
  triton.bind(
      model_name="Linear",
      infer_func=_infer_function_factory(devices=["cuda", "cpu"]),
      inputs=[
          Tensor(dtype=np.float32, shape=(-1,)),
      ],
      outputs=[
          Tensor(dtype=np.float32, shape=(-1,)),
      ],
      config=ModelConfig(max_batch_size=16),
  )
  ...
```

### Defining Inputs and Outputs

å¦‚ä½•åœ¨ä½¿ç”¨ Triton Inference Server æ—¶éœ€è¦å®šä¹‰ input å’Œ output çš„ shapeï¼Œæ•°æ®ç±»å‹ï¼ŒPyTriton åŒæ ·éœ€è¦ä½ è¿›è¡Œå®šä¹‰ã€‚**è¿™æ˜¯æˆ‘åœ¨æœ€åˆè¿›è¡Œå®è·µæ—¶é‡åˆ°è¿‡æœ€æ£˜æ‰‹çš„é—®é¢˜ã€‚å°¤å…¶æ˜¯å½“ input ä¸ºå­—ç¬¦ä¸²ç±»å‹çš„ prompt æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥å‚è€ƒ official example æ¥è§£å†³å­—ç¬¦ä¸² prompt input å¯èƒ½çš„é—®é¢˜ã€‚**

ä¸¾ä¸ªç®€å•çš„ä¾‹å­ï¼š

```python
import numpy as np
from pytriton.model_config import Tensor

inputs = [
    Tensor(dtype=np.float32, shape=(-1,)),
]
output = [
    Tensor(dtype=np.float32, shape=(-1,)),
    Tensor(dtype=np.int32, shape=(-1,)),
]
```

è¿™é‡Œå°±å®šä¹‰äº†ä¸€ä¸ªè¾“å…¥å’Œä¸¤ä¸ªè¾“å‡ºã€‚å¹¶ä¸”éœ€è¦æ³¨æ„çš„æ˜¯ï¼š`-1` ä»£è¡¨åŠ¨æ€çš„è¾“å…¥æˆ–è¾“å‡º shapeï¼ˆ**æ³¨æ„è¿™é‡Œçš„ `-1` å¹¶ä¸æ„å‘³ç€æ”¯æŒ batch**ï¼‰ï¼Œå¦‚æœæƒ³è¦å‡†ç¡®çš„å®šä¹‰ shapeï¼Œåˆ™éœ€è¦å…¨éƒ¨å†™å…¨ï¼Œä¾‹å¦‚ï¼š`Tensor(name="image", dtype=np.float32, shape=(224, 224, 3))`ã€‚

è€Œ `dtype` å‚æ•°å¯ä»¥æ˜¯`numpy.dtype`ï¼Œ`numpy.dtype.type` æˆ–è€… `str`ï¼Œä¾‹å¦‚ï¼š

```python
import numpy as np
from pytriton.model_config import Tensor

tensor1 = Tensor(name="tensor1", shape=(-1,), dtype=np.float32),
tensor2 = Tensor(name="tensor2", shape=(-1,), dtype=np.float32().dtype),
tensor3 = Tensor(name="tensor3", shape=(-1,), dtype="float32"),
```

> å½“ä½¿ç”¨ `bytes` æ•°æ®ç±»å‹æ—¶ï¼ŒNumPyä¼šåˆ é™¤å°¾éšçš„ `\x00` å­—èŠ‚ã€‚å› æ­¤ï¼Œå¦‚æœå½“æ—¶æ•°æ®å¯èƒ½å‡ºç°ä»»ä½•å­—èŠ‚ï¼ˆåŸæ–‡ä¸­è¯´çš„æ˜¯ arbitrary bytesï¼‰ï¼Œéœ€è¦ä½¿ç”¨ `object` æ•°æ®ç±»å‹ã€‚
>
> ```python
> > np.array([b"\xff\x00"])
> array([b'\xff'], dtype='|S2')
> 
> > np.array([b"\xff\x00"], dtype=object)
> array([b'\xff\x00'], dtype=object)
> ```
>
> **è¿™é‡Œçš„ `|S2` æœ‰å°ä¼™ä¼´å¯èƒ½æ¯”è¾ƒå›°æƒ‘ï¼Œå…¶å®å°±ä»£è¡¨æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ª `bytes` ç±»å‹ã€‚å…¶ä¸­ 2 ä»£è¡¨æœ€é•¿çš„å­—èŠ‚æ•°ï¼Œä¸ç†è§£çš„è¯·ç¨å¾®å¾€ä¸‹æµè§ˆï¼Œåç»­è¿˜ä¼šé€šè¿‡ example è¿›è¡Œè§£é‡Šã€‚** 
>
> ä½†æ˜¯ï¼Œå¦‚æœå½“æˆ‘ä»¬æ˜¯å¯¹ `string` è¿›è¡Œç¼–ç æ—¶ï¼Œä¸ºäº†æ–¹ä¾¿æˆ‘ä»¬**å¯ä»¥ä½¿ç”¨ `bytes` ç±»å‹**ã€‚
>
> é¦–å…ˆï¼Œ`\x00` ä»£è¡¨ç©ºå­—ç¬¦ï¼Œè€Œæ ‡å‡†æ–‡æœ¬å­—ç¬¦ä¸²ï¼ˆå¦‚UTF-8ç¼–ç çš„å­—ç¬¦ä¸²ï¼‰ä¸€èˆ¬ä¸åŒ…å« `\x00` å­—èŠ‚ã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨å¤„ç†**promptæ—¶å¯ä»¥æ”¾å¿ƒçš„ä½¿ç”¨ `bytes`ï¼Œå¯¹äºéæ–‡æœ¬æ•°æ®ï¼ˆå¦‚äºŒè¿›åˆ¶æ–‡ä»¶å†…å®¹ã€æŸäº›ç‰¹æ®Šæ ¼å¼çš„å­—ç¬¦ä¸²æˆ–å…¶ä»–ä»»æ„å­—èŠ‚æ•°æ®ï¼‰ï¼Œåœ¨è½¬æ¢ä¸º `bytes` æ—¶å¯èƒ½ä¼šåŒ…å« `\x00` å­—èŠ‚ã€‚è¿™ç±»æ•°æ®ä¸­çš„ `\x00` å­—èŠ‚å¯èƒ½æ˜¯æ•°æ®çš„ä¸€éƒ¨åˆ†ï¼Œå› æ­¤åœ¨è¿™ç§æƒ…å†µä¸‹ä½¿ç”¨ `object` ç±»å‹æ›´ä¸ºå®‰å…¨ï¼Œä»¥é¿å…åœ¨å¤„ç†ï¼ˆå¦‚å­˜å‚¨æˆ–ä¼ è¾“ï¼‰æ—¶ä¸¢å¤±è¿™äº›é‡è¦çš„å­—èŠ‚ã€‚**
>
> åœ¨ Python ä¸­ï¼Œ`bytes` ç±»å‹æ˜¯ä¸€ä¸ªä¸å¯å˜çš„åºåˆ—ï¼Œç”¨äºå­˜å‚¨å­—èŠ‚ï¼ˆå³0åˆ°255èŒƒå›´å†…çš„æ•´æ•°ï¼‰ã€‚å®ƒé€šå¸¸ç”¨äºå¤„ç†äºŒè¿›åˆ¶æ•°æ®ï¼Œå¦‚æ–‡ä»¶è¯»å†™ã€ç½‘ç»œé€šä¿¡ä¸­çš„æ•°æ®ä¼ è¾“ï¼Œä»¥åŠåœ¨å¤„ç†åŸå§‹æ•°æ®æ—¶ï¼ˆå¦‚å›¾åƒæˆ–å£°éŸ³æ–‡ä»¶çš„å†…å®¹ï¼‰ã€‚ã€GPT4è§£é‡Šã€‘

æ‰€ä»¥å½“æˆ‘ä»¬å¤„ç† prompt è¾“å…¥æ—¶ï¼Œå¯ä»¥å‚è€ƒ [NVIDIA official example: hugging face bart pytorch](https://github.com/triton-inference-server/pytriton/blob/main/examples/huggingface_bart_pytorch/client.py#L61-L68)ã€‚æˆ‘ä»¬ä½¿ç”¨ [`np.char.encode`](https://numpy.org/doc/stable/reference/generated/numpy.char.encode.html) ä¸º sequence ä¸­çš„æ¯ä¸ªå­—ç¬¦æ¬¡è¿›è¡Œç¼–ç ã€‚è¿™é‡Œçš„ 48 ä»£è¡¨çš„ä¾¿æ˜¯ np æ•°ç»„ä¸­æœ€é•¿çš„å­—ç¬¦ä¸²æ‰€å å­—ç¬¦æ•°ã€‚

```python
sequence = np.array(
    [
        ["one day I will see the world"],
        ["I would love to learn cook the Asian street food"],
        ["Carnival in Rio de Janeiro"],
        ["William Shakespeare was a great writer"],
    ]
)
"""
> sequence
array([['one day I will see the world'],
       ['I would love to learn cook the Asian street food'],
       ['Carnival in Rio de Janeiro'],
       ['William Shakespeare was a great writer']], dtype='<U48')
"""
sequence = np.char.encode(sequence, "utf-8")
"""
> sequence
array([[b'one day I will see the world'],
       [b'I would love to learn cook the Asian street food'],
       [b'Carnival in Rio de Janeiro'],
       [b'William Shakespeare was a great writer']], dtype='|S48')
"""
```

è€Œ Triton æœåŠ¡ç«¯ç«¯ input å’Œ output è®¾ç½®å¦‚ä¸‹æ‰€ç¤ºï¼Œå¹¶ä¸æ˜¯ `(4,1)` ï¼Œå› ä¸º `4` æ˜¯ batch æ•°ï¼Œç”± `infer_fn` ä¸­çš„ `@batch` ä¿®é¥°å™¨å¤„ç†ã€‚

```python
inputs=[Tensor(name="sequence", dtype=bytes, shape=(1,))],
outputs=[Tensor(name="label", dtype=bytes, shape=(1,))],
```



## BUG

è¿™é‡Œæ”¶é›†äº†ä¸€äº› Installation å’Œ Runtime ä¸­å‡ºç°çš„ bugã€‚

```
$ python example_1.py
...
/root/.cache/pytriton/workspace_oukek3b2/tritonserver/bin/tritonserver: /opt/conda/envs/MS/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /root/.cache/pytriton/workspace_oukek3b2/tritonserver/bin/tritonserver)
...
```

è¿™ä¸ª Issue æåˆ°äº†è¿™ä¸ªé—®é¢˜[Github Issue: version `GLIBCXX_3.4.30` not found](https://github.com/triton-inference-server/server/issues/5933)ã€‚å®ƒè¯´æ˜æˆ‘ä»¬ç³»ç»Ÿä¸­ç¼ºå°‘ç‰¹å®šç‰ˆæœ¬çš„GLIBCXXåº“ï¼Œæˆ–è€…ç³»ç»Ÿä¸­çš„åº“ç‰ˆæœ¬ä½äºæ‰€éœ€ç‰ˆæœ¬ã€‚

è§£å†³æ–¹æ³•æ˜¯åœ¨å¯¹åº”çš„ Conda è™šæ‹Ÿç¯å¢ƒä¸‹æ‰§è¡Œ `conda install -c conda-forge libstdcxx-ng=12 -y`ã€‚å…¶ä¸­ `-y` ä»£è¡¨é€‰æ‹©å…¨éƒ¨ä¸º `yes`ã€‚

> æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `strings /opt/conda/envs/MS/lib
> /libstdc++.so.6 | grep GLIBCXX` æ¥æŸ¥çœ‹å½“å‰åŠ¨æ€åº“æ”¯æŒçš„ GLIBCXX ç‰ˆæœ¬ã€‚
>
> è§£å†³æ–¹æ³•ä¸­æåˆ°çš„æœ¯è¯­åŒ…æ‹¬ï¼š
>
> 1. **GLIBCXX**ï¼š`GLIBCXX` æ˜¯ GNU C++ Standard Libraryï¼ˆæ ‡å‡†C++åº“ï¼‰çš„ä¸€éƒ¨åˆ†ï¼Œæ˜¯ GCCï¼ˆGNU Compiler Collectionï¼‰çš„ä¸€éƒ¨åˆ†ã€‚ä¸åŒç‰ˆæœ¬çš„ GCC é™„å¸¦ä¸åŒç‰ˆæœ¬çš„ GLIBCXX åº“ã€‚å½“è½¯ä»¶æˆ–ä»£ç ç¼–è¯‘æ—¶ï¼Œå®ƒä»¬å¯èƒ½ä¾èµ–äºç‰¹å®šç‰ˆæœ¬çš„ GLIBCXXï¼Œå› æ­¤è¿è¡Œè¿™äº›ç¨‹åºæ—¶éœ€è¦ç¡®ä¿ç³»ç»Ÿå…·æœ‰ç›¸åº”ç‰ˆæœ¬çš„ GLIBCXXã€‚
>
> 2. **libstdcxx-ng**ï¼š`libstdcxx-ng` æ˜¯åœ¨ Conda ç”Ÿæ€ç³»ç»Ÿä¸­æä¾›çš„ GNU C++ Standard Library çš„åŒ…ã€‚å®ƒæ˜¯ GCC çš„ C++ åº“çš„ Conda ç‰ˆæœ¬ï¼ŒåŒ…æ‹¬ `libstdc++.so` å…±äº«åº“ã€‚é€šè¿‡å®‰è£…ç‰¹å®šç‰ˆæœ¬çš„ `libstdcxx-ng` åŒ…ï¼Œå¯ä»¥åœ¨ Conda ç¯å¢ƒä¸­æä¾›å¯¹åº”ç‰ˆæœ¬çš„ GLIBCXXã€‚

## vLLM optimization

> è¿™ä¸ªè¿˜æŒºé‡è¦çš„ï¼Œæ„Ÿè§‰èƒ½å‡º 2ï½3 ä¸ªPRã€‚
>
> å¦‚æœå¯ä»¥æœ€åè¿˜å¯ä»¥å®é™…çš„æµ‹ä¸€ä¸‹æ€§èƒ½çš„å¯¹æ¯”ã€‚

æˆ‘æœ‰ä¸€ä¸ªå›°æƒ‘æ˜¯ PyTriton + vLLM çš„ç»„åˆèƒ½å¦ä¸€èµ·ä½¿ç”¨å‘¢ï¼Ÿç»è¿‡éªŒè¯è¿™å®é™…ä¸Šæ˜¯å¯è¡Œçš„ã€‚

å¯ä»¥å‚è€ƒçš„æ–‡çŒ®ï¼š

- [Binding Models to Triton](https://triton-inference-server.github.io/pytriton/latest/binding_models/)
- [NVIDIA PyTriton Example: vLLM](https://github.com/triton-inference-server/pytriton/tree/main/examples/vllm)
  - é—®é¢˜ä¸€ï¼Œæ²¡æœ‰æ¸…æ™°çš„å±•ç¤ºå•ä¸€ prompt å’Œ batch prompt çš„ä½¿ç”¨åŒºåˆ«ï¼Ÿ
- [NVIDIA PyTriton Example: BART-PyTorch](https://github.com/triton-inference-server/pytriton/tree/main/examples/huggingface_bart_pytorch)
  - å±•ç¤ºäº†ï¼Œä½†æ˜¯ç›´æ¥ä½¿ç”¨åœ¨ vLLM ä¸­ä¼šå‡ºç°é—®é¢˜ã€‚

```

```

# Reference

- [NVIDIA Triton æ¨ç†æœåŠ¡å™¨](https://developer.nvidia.cn/triton-inference-server)
- [NVIDIA AI Triton æ¨ç†æœåŠ¡å™¨ï¼ˆåŒä¸Šé¢å†…å®¹ç±»ä¼¼ï¼‰](https://www.nvidia.cn/ai-data-science/products/triton-inference-server/) 
- [NVIDIA Triton Inference Server](https://www.nvidia.com/en-us/ai-data-science/products/triton-inference-server/)
- [Blog: Deploying Llama2 with NVIDIA Triton Inference Server](https://blog.marvik.ai/2023/10/16/deploying-llama2-with-nvidia-triton-inference-server/)
- [Fast and Scalable AI Model Deployment with NVIDIA Triton Inference Serverï¼ˆä»‹ç»æ•´ä½“çš„ Triton ç‰¹æ€§ï¼‰](https://developer.nvidia.com/blog/fast-and-scalable-ai-model-deployment-with-nvidia-triton-inference-server/)
