# NVIDIA Triton Inference Server

*Deploy, run, and scale AI for any application on any platform.*

ä¸»è¦æœ‰ä¸¤ä¸ª topic éœ€è¦æ¢ç´¢ï¼š

- Triton Inference Server + Client
- PyTriton

æˆ‘ä»¬éœ€è¦åˆ†åˆ«ææ¸…æ¥šä»–ä»¬æ˜¯ä»€ä¹ˆï¼Ÿå¯¹ PyTriton æœ‰å…´è¶£çš„å¯ä»¥ç›´æ¥è·³è½¬åˆ° PyTriton ç« èŠ‚ã€‚

>  NVIDIA Tritonâ„¢ æ¨ç†æœåŠ¡å™¨æ˜¯ NVIDIA AI å¹³å°çš„ä¸€éƒ¨åˆ†ï¼Œå®ƒæ˜¯ä¸€æ¬¾å¼€æºæ¨ç†æœåŠ¡è½¯ä»¶ï¼Œå¯åŠ©åŠ›æ ‡å‡†åŒ–æ¨¡å‹çš„éƒ¨ç½²å’Œæ‰§è¡Œï¼Œå¹¶åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æä¾›å¿«é€Ÿä¸”å¯æ‰©å±•çš„ AIã€‚

## Introduction

### What's NVIDIA Triton?

é¦–å…ˆä¸€ä¸ªä¸»è¦é—®é¢˜æ˜¯ï¼šä»€ä¹ˆæ˜¯ Tritonï¼Ÿ

> Triton å°±æ˜¯ä¸€ä¸ªæ¨ç†æœåŠ¡å™¨ï¼Œæ—¨åœ¨å®ç°ç»Ÿä¸€çš„ AI æ¨ç†ã€‚ä¹Ÿå°±æ˜¯æ»¡è¶³â€œä»»ä½•â€AIæ¨ç†éœ€æ±‚ã€‚
>
> è¿™æ˜¯æ›´è¯¦ç»†çš„ï¼šTriton æ¨ç†æœåŠ¡å™¨å¯åŠ©åŠ›å›¢é˜Ÿåœ¨ä»»æ„åŸºäº GPU æˆ– CPU çš„åŸºç¡€è®¾æ–½ä¸Šéƒ¨ç½²ã€è¿è¡Œå’Œæ‰©å±•ä»»æ„æ¡†æ¶ä¸­ç»è¿‡è®­ç»ƒçš„ AI æ¨¡å‹ï¼Œè¿›è€Œç²¾ç®€ AI æ¨ç†ã€‚åŒæ—¶ï¼ŒAI ç ”ç©¶äººå‘˜å’Œæ•°æ®ç§‘å­¦å®¶å¯åœ¨ä¸å½±å“ç”Ÿäº§éƒ¨ç½²çš„æƒ…å†µä¸‹ï¼Œé’ˆå¯¹å…¶é¡¹ç›®è‡ªç”±é€‰æ‹©åˆé€‚çš„æ¡†æ¶ã€‚å®ƒè¿˜å¸®åŠ©å¼€å‘è€…è·¨äº‘ã€æœ¬åœ°ã€è¾¹ç¼˜å’ŒåµŒå…¥å¼è®¾å¤‡æä¾›é«˜æ€§èƒ½æ¨ç†ã€‚

### Triton Inference Server Features

è¿™ä¸ªé—®é¢˜ç­‰ä»·äºï¼š **Why NVIDIA Tritonï¼Ÿ**

![triton-adventages](./assets/triton-adventages.png)

Triton çš„ä¸»è¦ä¼˜åŠ¿å¦‚ä¸Šå›¾ï¼Œ**å¦‚æœå¯¹äºè¿™éƒ¨åˆ†ä¸æ„Ÿå…´è¶£å¯ä»¥å¿«é€Ÿçš„è·³åˆ°åç»­çš„ç« èŠ‚**ã€‚æ¥ä¸‹æ¥æˆ‘éœ€è¦ææ¸…æ¥š Triton çš„ä¸»è¦ä¼˜åŠ¿ï¼Œä¸ºä»€ä¹ˆ Triton Server ä¼šè¢«è¿™ä¹ˆå¤šä¸“ä¸šçš„äº§å“ï¼Œå›¢é˜Ÿæ‰€ä½¿ç”¨ã€‚

- **ğŸ”¥ æ”¯æŒå¤šä¸ªæ¡†æ¶ï¼ˆSupports All Training and Inference Frameworksï¼‰**
  - Triton æ¨ç†æœåŠ¡å™¨æ”¯æŒæ‰€æœ‰ä¸»æµæ¡†æ¶ï¼Œä¾‹å¦‚ TensorFlowã€NVIDIAÂ® TensorRTâ„¢ã€PyTorchã€MXNetã€Pythonã€ONNXã€RAPIDSâ„¢ FILï¼ˆç”¨äº XGBoostã€scikit-learn ç­‰ï¼‰ã€OpenVINOã€è‡ªå®šä¹‰ C++ ç­‰ã€‚
  - **é‡è¦æ€§ï¼š**è¿™ä¸ªæ¯«æ— ç–‘é—®æ˜¯æœ€é‡è¦ï¼Œä¹Ÿæ˜¯ Triton æœ€å—æ¬¢è¿çš„ç‰¹æ•ˆä¹‹ä¸€ï¼Œæ”¯æŒå¤§é‡çš„æ¡†æ¶ã€‚
- **ğŸš€ é«˜æ€§èƒ½æ¨ç†ï¼ˆHigh-Performance Inferenceï¼‰**
  - ä¸­æ–‡ï¼šTriton æ”¯æŒæ‰€æœ‰åŸºäº NVIDIA GPUã€x86 å’Œ ARMÂ® CPU çš„æ¨ç†ã€‚å®ƒå…·æœ‰**åŠ¨æ€æ‰¹å¤„ç†**ã€**å¹¶å‘æ‰§è¡Œ**ã€**æœ€ä¼˜æ¨¡å‹é…ç½®**ã€**æ¨¡å‹é›†æˆ**å’Œ**ä¸²æµè¾“å…¥**ç­‰åŠŸèƒ½ï¼Œå¯æ›´å¤§é™åº¦åœ°æé«˜ååé‡å’Œåˆ©ç”¨ç‡ã€‚
  - English: Maximize throughput and utilization with **dynamic batching**, **concurrent execution**, **optimal configuration**, and **streaming audio and video**. Triton Inference Server supports all NVIDIA GPUs, x86 and ArmÂ® CPUs, and AWS Inferentia.
  - **é‡è¦æ€§ï¼š** æ¨ç†æœåŠ¡å™¨ï¼Œæœ€é‡è¦çš„å°±æ˜¯æ¨ç†æ€§èƒ½ï¼ŒNVIDAI ä½œä¸º AI è®¡ç®—çš„è¡Œä¸šé¾™å¤´ Triton åœ¨æ€§èƒ½è¿™å—ä¼˜åŠ¿è‚¯å®šéå¸¸æ˜¾è‘—ã€‚
- ä¸“ä¸º DevOps å’Œ MLOps è®¾è®¡ï¼ˆè¿™æ®µæš‚æ—¶å¿½ç•¥ ï¼Œæ„Ÿè§‰å’Œäº‘è®¡ç®—é‚£ä¸€å¥—æ¯”è¾ƒæ²¾è¾¹ï¼‰
  - Triton ä¸ Kubernetes é›†æˆï¼Œå¯ç”¨äºç¼–æ’å’Œæ‰©å±•ï¼Œå¯¼å‡º Prometheus æŒ‡æ ‡è¿›è¡Œç›‘æ§ï¼Œæ”¯æŒå®æ—¶æ¨¡å‹æ›´æ–°ï¼Œå¹¶å¯ç”¨äºæ‰€æœ‰ä¸»æµçš„å…¬æœ‰äº‘ AI å’Œ Kubernetes å¹³å°ã€‚å®ƒè¿˜ä¸è®¸å¤š MLOPS è½¯ä»¶è§£å†³æ–¹æ¡ˆé›†æˆã€‚

> å¯¹äºç”¨æˆ·æ¥è¯´ï¼Œæˆ‘ä»¬å¯èƒ½æ›´å…³ç³»çš„æ˜¯é«˜æ€§èƒ½æ¨ç†éƒ¨åˆ†æ‰€ä½¿ç”¨çš„æŠ€æœ¯ç»†èŠ‚ï¼ˆæˆ‘å°±æ˜¯è¿™æ ·ï¼‰ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç©¶ã€‚æˆ‘ä»¬å°†ä»‹ç»ï¼š
>
> - [x] Dynamic Batching
> - [x] Concurrent Execution
> - [ ] Optimal Configuration
> - [ ] Model Ensemble
> - [ ] Streaming audio and Vedio
>
> ### Dynamic Batching
>
> Batchå¯¹äºGPUä¸Šæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è¿è¡Œæ•ˆç‡å½±å“å¾ˆå¤§ã€‚åœ¨ Inference æ—¶ï¼Œå¦‚æœåŒæ—¶é¢ä¸´å¤šä¸ªè¯·æ±‚ï¼Œå¦‚æœæƒ³æå‡ååé‡ï¼Œéœ€è¦æˆ‘ä»¬æŠŠè¿™å¤šä¸ªè¯·æ±‚ç»„åˆæˆä¸€ä¸ª batch ç”± GPU è¿›è¡Œ Batch Inferenceã€‚æ‰€ä»¥ Triton ä¸­çš„ Dynamic Batching ä¾¿æ˜¯å®ç°äº†è¿™æ ·çš„åŠŸèƒ½ã€‚
>
> Triton æ”¯æŒè®¾ç½® `max batch size` å¤§å°å’Œ `delay` åŒºé—´ã€‚æˆ‘ä»¬é€šè¿‡ä¸‹å›¾å¯ä»¥å‘ç°å¼€å¯ Dynamic Batching å¹¶è®¾ç½®åˆç†çš„ Delay å¯ä»¥å¤§å¹…å¢å¼ºååé‡ã€‚
>
> <img src="./assets/dynamic_batching.png" alt="img" style="zoom: 50%;" />
>
> ### Concurrent Execution
>
> Concurrent Execution åœ¨æˆ‘çœ‹æ¥å¯ä»¥ç«‹å³ä¸º**è‡ªåŠ¨ç®¡ç†å¤šçº¿ç¨‹æ‰§è¡Œæ¨¡å‹æ¨ç†ä»»åŠ¡ã€‚**
>
> å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬åœ¨ GPU ä¸Šå¼€äº† 4 ä¸ªå®ä¾‹ï¼Œå…¶ä¸­ 1 ä¸ªç”¨æ¥è¿›è¡Œ Model0 çš„æ¨ç†ï¼Œ3 ä¸ªç”¨æ¥è¿›è¡Œ Model1 çš„æ¨ç†ã€‚
>
> ![concurrent-execution](./assets/concurrent-execution.png)
>
> ä¸‹å›¾ä¾¿æ˜¯ Triton åŒæ—¶ä½¿ç”¨ Dynamic Batching å’Œ Concurrent Executionã€‚ä¸Šä¸‹å¯¹æ¯”æˆ‘ä»¬å‘ç°æ€§èƒ½è¿›ä¸€æ­¥æå‡ã€‚
>
> <img src="./assets/dynamic-batch-and-concurrent-execution.png" alt="dynamic-batch-and-concurrent-execution" style="zoom:50%;" />
>
> å‰©ä½™æ²¡æœ‰æ¢ç´¢çš„å†…å®¹å¯ä»¥å‚è€ƒå¼•ç”¨å¦‚ä¸‹ï¼š
>
> - [çŸ¥ä¹ï¼šDynamic Batchingï¼](https://zhuanlan.zhihu.com/p/354633729)
> - [ğŸ”¥ triton-inference-serverçš„backendï¼ˆä¸€ï¼‰â€”â€”å…³äºæ¨ç†æ¡†æ¶çš„ä¸€äº›è®¨è®º](https://zhuanlan.zhihu.com/p/666655108)
> - [Model Ensemble: Serving ML Model Pipelines on NVIDIA Triton Inference Server with Ensemble Models](https://developer.nvidia.com/blog/serving-ml-model-pipelines-on-nvidia-triton-inference-server-with-ensemble-models/)
> - [NVIDIA: Concurrent inference and dynamic batching](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/examples/jetson/concurrency_and_dynamic_batching/README.html)
> - [**æœ€ä¼˜æ¨¡å‹é…ç½®**: NVIDIA Triton Model Analyzer](https://github.com/triton-inference-server/model_analyzer)

## Install

å¦‚ä½•è¿›è¡Œå®‰è£…ã€‚

## Example å®æˆ˜ Llama-7b



# ğŸ”¥PyTriton

## Installation

ä¸»è¦å†…å®¹å¯ä»¥å‚è€ƒ NVIDAI çš„å®˜æ–¹æ–‡æ¡£ï¼š[Offical Installation](https://triton-inference-server.github.io/pytriton/latest/installation/)ã€‚ä¾æ®å›½å†…çš„ä¹ æƒ¯ï¼Œæˆ‘æ¨èå‚è€ƒ Creating virtualenv using `miniconda` è¿™ä¸ª partã€‚æˆ‘è‡ªå·±çš„å®‰è£…ç»éªŒæ˜¯ï¼š

```bash
# Create your virtual environment.
...

# Export the conda library path
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib

pip install -U nvidia-pytriton
```

ï¼ˆä¸‹é¢çš„å†…å®¹å¯ä»¥è·³è¿‡ï¼‰ä¹‹æ‰€ä»¥éœ€è¦è¿™ä¸ª `export` è¯­å¥æ˜¯å› ä¸ºåœ¨å®‰è£…æ—¶é‡åˆ°äº†ä¸€ä¸ªå° BUGï¼Œåœ¨å®‰è£…åè¿è¡Œäº†ä¸€æ®µ Example ä»£ç ä½†æ˜¯é‡åˆ°æŠ¥é”™ä¿¡æ¯ï¼š

```
I0212 16:32:55.775014 9771 model_lifecycle.cc:461] loading: Linear:1
/root/.cache/pytriton/workspace_qgciftzs/tritonserver/backends/python/triton_python_backend_stub: error while loading shared libraries: libpython3.9.so.1.0: cannot open shared object file: No such file or directory
```

å¤§æ¦‚æ„æ€æ˜¯ `libpython3.9.so.1.0` è¿™ä¹ˆä¸€ä¸ªåŠ¨æ€é“¾æ¥åº“æ²¡æ‰¾åˆ°ã€‚å…¶å® Official Installation é‡Œé¢ä¹Ÿæœ‰å…³äºè¿™éƒ¨åˆ†çš„è¯´æ˜ã€‚æˆ‘ä»¬åªéœ€è¦åœ¨å½“å‰çš„ SHELL ä¸­æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ã€‚

```bash
# Export the conda library path
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib
```

> å…¶ä¸­ `CONDA_PREFIX` æ˜¯ä¸€ä¸ªç¯å¢ƒå˜é‡ï¼Œå®ƒè¡¨ç¤ºå½“å‰æ¿€æ´»çš„condaç¯å¢ƒçš„è·¯å¾„ã€‚å½“ä½ ä½¿ç”¨condaæ¿€æ´»ä¸€ä¸ªç‰¹å®šçš„ç¯å¢ƒæ—¶ï¼Œcondaä¼šè‡ªåŠ¨è®¾ç½®`CONDA_PREFIX`å˜é‡æ¥æŒ‡å‘é‚£ä¸ªç¯å¢ƒçš„æ ¹ç›®å½•ã€‚è€Œå…³äº **LD_LIBRARY_PATH** æ˜¯ä»€ä¹ˆå¯ä»¥å‚è€ƒæˆ‘çš„ä»“åº“ä¸­çš„ [`env/cuda-related.md`](https://github.com/keli-wen/AGI-Study/blob/master/env/cuda-related.md#3-multi-cuda-management)ã€‚

**æ³¨æ„æˆ‘ä»¬ä¸èƒ½ç›´æ¥åœ¨ `~/.bashrc` æˆ–è€… `~/.zshrc` ä¸­æ·»åŠ ä¸Šè¿°å‘½ä»¤**ã€‚å› ä¸ºæˆ‘ä»¬éœ€è¦ source ä¿è¯æ›´æ–°è¢«åº”ç”¨ï¼Œä½†æ˜¯**é€šå¸¸æƒ…å†µ**ä¸‹ source ä¼šæ”¹å˜å½“å‰ conda è™šæ‹Ÿç¯å¢ƒï¼Œä½¿å¾— `CONDA_PREFIX` æ’å®šä¸º `/opt/conda`ï¼ˆä¹Ÿå°±æ˜¯ base ç¯å¢ƒï¼‰ã€‚

## Introduction

> ä¸€ä¸ªåˆšå‘å¸ƒä¸ä¹…çš„æ¡†æ¶ã€‚å®ƒå…¶å®å°±æ˜¯è®©æˆ‘ä»¬å¯ä»¥åœ¨ Python ç«¯ç›´æ¥å¯åŠ¨ Triton Inference Serverã€‚ 

â€œPyTriton is a Flask/FastAPI-like interface that simplifies Triton's deployment in Python environments. The library allows serving Machine Learning models directly from Python through NVIDIA's [Triton Inference Server](https://github.com/triton-inference-server). The solution is framework-agnostic and can be used along with frameworks like PyTorch, TensorFlow, or JAX.â€

PyTriton æä¾›ä¸€ç§é€‰æ‹©è®©ä½ çš„ Python æ¨¡å‹å¯ä»¥ä½¿ç”¨ Triton Inference Server æ¥å¤„ç† HTTP/gRPC è¯·æ±‚ã€‚æˆ‘ä»¬ä½¿ç”¨**é˜»å¡æ¨¡å¼**ï¼Œä¹Ÿå°±æ˜¯è¿™ä¸ªç¨‹åºä¼šé•¿æœŸè¿è¡Œåœ¨ä½ éƒ¨ç½²çš„é›†ç¾¤ä¸Šæ¥å¤„ç†æ¥è‡ª client çš„å„ç§è¯·æ±‚ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬è¦å®šä¹‰å¥½ä¸€ä¸ª `infer_fn` å›è°ƒå‡½æ•°ç”¨æ¥å¤„ç† inferenceã€‚å¦‚ä¸‹ï¼Œè¿™é‡Œçš„ä¿®é¥°å™¨ `@batch` ä»£è¡¨è¯¥æ¨¡å‹æ¥å— batch è¾“å…¥ã€‚

```py
import numpy as np
from pytriton.decorators import batch


@batch
def infer_fn(**inputs: np.ndarray):
    input1, input2 = inputs.values()
    outputs = model(input1, input2)
    return [outputs]
```

ä¸‹ä¸€æ­¥åˆ™æ˜¯é€šè¿‡ä»£ç å°† Triton å’Œ `infer_fn` è¿æ¥ã€‚åªéœ€è¦ `bind` å‡½æ•°ä¾¿å¯ä»¥å®ç°ã€‚

åœ¨é˜»å¡æ¨¡å¼ä¸‹ï¼Œå»ºè®®ä½¿ç”¨ Triton å¯¹è±¡ä½œä¸ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚ï¼ˆä¸ºä»€ä¹ˆæˆ‘ä¹Ÿæ²¡ç†è§£ï¼‰

é€šè¿‡å¦‚ä¸‹ä»£ç ä¾¿å·²ç»å®šä¹‰å¥½äº† Triton è¯¥å¦‚ä½•å¤„ç†æ¨¡å‹ä»¥åŠ HTTP/gRPC è¯·æ±‚åº”è¢«å®šå‘åˆ°ä½•å¤„ã€‚

```python
with Triton() as triton:
    triton.bind(
        model_name="MyModel",
        infer_func=infer_fn,
        inputs=[
            Tensor(dtype=bytes, shape=(1,)),  # sample containing single bytes value
            Tensor(dtype=bytes, shape=(-1,)),  # sample containing vector of bytes
        ],
        outputs=[
            Tensor(dtype=np.float32, shape=(-1,)),
        ],
        config=ModelConfig(max_batch_size=16),
    )
    ...
    triton.serve()
```

æœ€åä¸€æ­¥ä¹‹éœ€è¦ä½¿ç”¨ `triton.serve()` ä¾¿å¯åŠ¨äº† Triton Inference Serveï¼Œæ‰€æœ‰çš„è¯·æ±‚ä¼šè¢«é‡å®šå‘åˆ° `localhost:8000/v2/models/MyModel/infer` å¹¶ç”± `infer_fn` æ‰§è¡Œæ¨ç†ä»»åŠ¡ã€‚

## Quick Start

å¦‚æœä½ æƒ³è¦å¿«é€Ÿäº†è§£ PyTriton çš„ä½¿ç”¨ï¼Œé˜…è¯» [Official: Quick Start](https://triton-inference-server.github.io/pytriton/latest/quick_start/) æ˜¯æœ€ç›´æ¥ã€‚**å®ƒåˆ†åˆ«ä»‹ç»äº† `server` å’Œ `client` çš„å®šä¹‰ä¸ä½¿ç”¨ã€‚**

å¯¹äº `server`ï¼Œä¸»è¦æ˜¯å¦‚ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

1. å®šä¹‰ `model`ï¼Œè¿™é‡Œæ˜¯ `Linear`ã€‚
2. å®šä¹‰ `infer_fn` ç”¨äºåç»­çš„ç»‘å®šæ“ä½œã€‚
3. é€šè¿‡ `triton.bind(*)` å’Œ `torch.serve()` ã€‚

```python
import numpy as np
import torch

from pytriton.decorators import batch
from pytriton.model_config import ModelConfig, Tensor
from pytriton.triton import Triton

model = torch.nn.Linear(2, 3).to("cuda").eval()


@batch
def infer_fn(**inputs: np.ndarray):
    (input1_batch,) = inputs.values()
    input1_batch_tensor = torch.from_numpy(input1_batch).to("cuda")
    output1_batch_tensor = model(
        input1_batch_tensor
    )  # Calling the Python model inference
    output1_batch = output1_batch_tensor.cpu().detach().numpy()
    return [output1_batch]


def main():
    # Connecting inference callable with Triton Inference Server
    with Triton() as triton:
        triton.bind(
            model_name="Linear",
            infer_func=infer_fn,
            inputs=[
                Tensor(dtype=np.float32, shape=(-1,)),
            ],
            outputs=[
                Tensor(dtype=np.float32, shape=(-1,)),
            ],
            config=ModelConfig(max_batch_size=128),
        )
        triton.serve()

if __name__ == "__main__":
    main()
```

å¯¹äº `client`ï¼Œå¯ä»¥é€šè¿‡ `curl` è¿›è¡Œè®¿é—®ï¼ˆå…·ä½“å¯ä»¥å‚è€ƒä¸Šé¢çš„é“¾æ¥ï¼‰ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬æ›´ç†Ÿæ‚‰çš„æ˜¯ä½¿ç”¨ Pythonã€‚

```python
import torch
from pytriton.client import ModelClient

input1_data = torch.randn(128, 2).cpu().detach().numpy()

with ModelClient("localhost:8000", "Linear") as client:
    result_dict = client.infer_batch(input1_data)

print(result_dict)
```

è¿™æ ·çœ‹æ¥ï¼ŒPyTriton ç¡®å®å¤§å¤§ç®€åŒ–äº† Triton Inference Server çš„ä½¿ç”¨æµç¨‹ã€‚

## BUG

è¿™é‡Œæ”¶é›†äº†ä¸€äº› Installation å’Œ Runtime ä¸­å‡ºç°çš„ bugã€‚

```
$ python example_1.py
...
/root/.cache/pytriton/workspace_oukek3b2/tritonserver/bin/tritonserver: /opt/conda/envs/MS/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /root/.cache/pytriton/workspace_oukek3b2/tritonserver/bin/tritonserver)
...
```

è¿™ä¸ª Issue æåˆ°äº†è¿™ä¸ªé—®é¢˜[Github Issue: version `GLIBCXX_3.4.30` not found](https://github.com/triton-inference-server/server/issues/5933)ã€‚å®ƒè¯´æ˜æˆ‘ä»¬ç³»ç»Ÿä¸­ç¼ºå°‘ç‰¹å®šç‰ˆæœ¬çš„GLIBCXXåº“ï¼Œæˆ–è€…ç³»ç»Ÿä¸­çš„åº“ç‰ˆæœ¬ä½äºæ‰€éœ€ç‰ˆæœ¬ã€‚

è§£å†³æ–¹æ³•æ˜¯åœ¨å¯¹åº”çš„ Conda è™šæ‹Ÿç¯å¢ƒä¸‹æ‰§è¡Œ `conda install -c conda-forge libstdcxx-ng=12 -y`ã€‚å…¶ä¸­ `-y` ä»£è¡¨é€‰æ‹©å…¨éƒ¨ä¸º `yes`ã€‚

> æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `strings /opt/conda/envs/MS/lib
> /libstdc++.so.6 | grep GLIBCXX` æ¥æŸ¥çœ‹å½“å‰åŠ¨æ€åº“æ”¯æŒçš„ GLIBCXX ç‰ˆæœ¬ã€‚
>
> è§£å†³æ–¹æ³•ä¸­æåˆ°çš„æœ¯è¯­åŒ…æ‹¬ï¼š
>
> 1. **GLIBCXX**ï¼š`GLIBCXX` æ˜¯ GNU C++ Standard Libraryï¼ˆæ ‡å‡†C++åº“ï¼‰çš„ä¸€éƒ¨åˆ†ï¼Œæ˜¯ GCCï¼ˆGNU Compiler Collectionï¼‰çš„ä¸€éƒ¨åˆ†ã€‚ä¸åŒç‰ˆæœ¬çš„ GCC é™„å¸¦ä¸åŒç‰ˆæœ¬çš„ GLIBCXX åº“ã€‚å½“è½¯ä»¶æˆ–ä»£ç ç¼–è¯‘æ—¶ï¼Œå®ƒä»¬å¯èƒ½ä¾èµ–äºç‰¹å®šç‰ˆæœ¬çš„ GLIBCXXï¼Œå› æ­¤è¿è¡Œè¿™äº›ç¨‹åºæ—¶éœ€è¦ç¡®ä¿ç³»ç»Ÿå…·æœ‰ç›¸åº”ç‰ˆæœ¬çš„ GLIBCXXã€‚
>
> 2. **libstdcxx-ng**ï¼š`libstdcxx-ng` æ˜¯åœ¨ Conda ç”Ÿæ€ç³»ç»Ÿä¸­æä¾›çš„ GNU C++ Standard Library çš„åŒ…ã€‚å®ƒæ˜¯ GCC çš„ C++ åº“çš„ Conda ç‰ˆæœ¬ï¼ŒåŒ…æ‹¬ `libstdc++.so` å…±äº«åº“ã€‚é€šè¿‡å®‰è£…ç‰¹å®šç‰ˆæœ¬çš„ `libstdcxx-ng` åŒ…ï¼Œå¯ä»¥åœ¨ Conda ç¯å¢ƒä¸­æä¾›å¯¹åº”ç‰ˆæœ¬çš„ GLIBCXXã€‚

# Reference

- [NVIDIA Triton æ¨ç†æœåŠ¡å™¨](https://developer.nvidia.cn/triton-inference-server)
- [NVIDIA AI Triton æ¨ç†æœåŠ¡å™¨ï¼ˆåŒä¸Šé¢å†…å®¹ç±»ä¼¼ï¼‰](https://www.nvidia.cn/ai-data-science/products/triton-inference-server/) 
- [NVIDIA Triton Inference Server](https://www.nvidia.com/en-us/ai-data-science/products/triton-inference-server/)
- [Blog: Deploying Llama2 with NVIDIA Triton Inference Server](https://blog.marvik.ai/2023/10/16/deploying-llama2-with-nvidia-triton-inference-server/)
- [Fast and Scalable AI Model Deployment with NVIDIA Triton Inference Serverï¼ˆä»‹ç»æ•´ä½“çš„ Triton ç‰¹æ€§ï¼‰](https://developer.nvidia.com/blog/fast-and-scalable-ai-model-deployment-with-nvidia-triton-inference-server/)
