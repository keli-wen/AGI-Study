# Basic LLM Inference/Generation

â° Read : `35min`

> æƒ­æ„§ï¼Œåœ¨æˆ‘é˜…è¯»å¾ˆå¤šå…¶ä»– LLM ç›¸å…³çš„æ–‡ç« æ—¶ï¼Œå‘ç°æˆ‘å¯¹ LLM çš„ Inference/Sampling çš„è¿‡ç¨‹ä¸å¤Ÿäº†è§£ã€‚åŸºç¡€ä¸ç‰¢ï¼Œåœ°åŠ¨å±±æ‘‡ã€‚æ‰€ä»¥æˆ‘å°è¯•é¦–å…ˆç†è§£åŸºç¡€çš„ LLM Inference Pipelineã€‚

## 0. Goal

æœ¬æ–‡æ—¨åœ¨å›ç­”ä¹‹åçš„ä¸¤ä¸ªé—®é¢˜ï¼š

- **Question1ï¼šè¾“å…¥ N ä¸ª tokens ï¼ˆpromptï¼‰ï¼ŒLLM æ˜¯å¦‚ä½•å¾—åˆ°ä¸‹ä¸€ä¸ª tokenï¼Ÿåˆæ˜¯å¦‚ä½•è¿›è¡Œè‡ªå›å½’é‡‡æ ·ï¼ˆAuto- regressive Samplingï¼‰å‘¢ï¼Ÿ**
- **Question2ï¼šLLM æ˜¯å¦‚ä½•å¤„ç†ä¸å®šé•¿çš„ batch inferenceï¼Ÿ**

## 1. Pre-knowledge

> åç»­ Inference/Generation æ¶‰åŠåˆ°çš„æœ€åŸºç¡€çš„å…ƒçŸ¥è¯†ã€‚ã€ğŸ“–é¢„è®¡ï¼š5minã€‘

### 1.1. Temperature

åœ¨è¿›è¡Œ generation çš„æ—¶å€™ï¼Œæˆ‘ä»¬æœ‰æ—¶å€™è¢«è¦æ±‚è®¾å®š temperature çš„å€¼ï¼Œé‚£ä¹ˆå®ƒåˆ°åº•æœ‰ä»€ä¹ˆä½œç”¨å‘¢ï¼Ÿ

é€šå¸¸æ¨¡å‹çš„è¾“å‡ºæ˜¯ä¸€äº›å€¼ï¼ˆlogitsï¼‰è€Œä¸æ˜¯åˆ†å¸ƒï¼ˆprobability distributionï¼‰ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶è½¬æ¢æˆåˆ†å¸ƒï¼Œè½¬æ¢é€šå¸¸ä½¿ç”¨çš„æ˜¯softmax å‡½æ•°ï¼š

$$
\dfrac{\exp(z_i)}{\sum \exp(z_j)}
$$

è™½ç„¶ Softmax å¯ä»¥å¾—åˆ°ä¸€ä¸ªåˆ†å¸ƒï¼Œä½†åŒæ—¶ä¹Ÿæœ‰å…¶ç¼ºç‚¹ã€‚å®¹æ˜“æ‰©å¤§/ç¼©å°å†…éƒ¨å…ƒç´ çš„å·®å¼‚ï¼ˆå¼‚åŒ–æˆ max / meanï¼‰ï¼Œå¦‚ï¼ˆè¿™é‡Œæ˜¯å€Ÿé‰´çš„ä¾‹å­ï¼‰ï¼š

- `[11, 12, 13]` è¿›è¡Œ softmax åä¸º `[0.0900, 0.2447, 0.6652]`ï¼Œ è¿™å¯¼è‡´æœ€ç»ˆé‡‡æ ·åçš„ç»“æœ**ä¸å¤Ÿä¸°å¯Œ**ã€‚

- `[0.01, 0.02, 0.03]` è¿›è¡Œ softmax åä¸º `[0.3300, 0.3333, 0.3367]`ï¼Œè¿™å°†å¯¼è‡´æœ€ç»ˆé‡‡æ ·æ–¹æ³•æ˜¯åœ¨éšæœºé‡‡æ ·ï¼Œ**ç”Ÿæˆä¸åˆç†çš„åºåˆ—**ã€‚

Temperature $T$ ä¾¿æ˜¯ç”¨æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç”¨äºè°ƒèŠ‚ softmax ï¼Œè®©å…¶åˆ†å¸ƒè¿›ä¸€æ­¥ç¬¦åˆæˆ‘ä»¬çš„é¢„æœŸã€‚

$$
\dfrac{\exp(z_i / T)}{\sum \exp(z_j / T)}
$$

å¦‚å›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬èƒ½å¿«é€Ÿçš„ç†è§£ T å¯¹äº Softmax åˆ†å¸ƒçš„å½±å“ã€‚

![img](./assets/temperature.gif)

- å½“ $T$ è¶Šå¤§ï¼Œåˆ†å¸ƒä¼šè¶Šè¶‹è¿‘äº uniform distributionï¼Œé‡‡æ ·ç»“æœçš„éšæœºæ€§è¶Šå¤§ã€‚
- å½“ $T$ è¶Šå°ï¼Œåˆ†å¸ƒä¼šè¶Šè¶‹è¿‘äº one-point distributionï¼Œé‡‡æ ·ç»“æœè¶Šè¶‹è¿‘äºä¸€è‡´ã€‚

æ‰€ä»¥ä¸ºä»€ä¹ˆå« temperature å‘¢ï¼Ÿæˆ‘ä»¬çŸ¥é“ï¼šæ¸©åº¦è¶Šé«˜ï¼Œå¸ƒæœ—è¿åŠ¨è¶Šå‰§çƒˆï¼›åŒç†ï¼Œtemperature è¶Šé«˜ï¼Œé‡‡æ ·å¾—åˆ°çš„ç»“æœè¶Šéšæœºã€‚

### Top-p/Nucleus Sampling

Top-p sampling æ ¸å¿ƒæ€æƒ³æ˜¯é€‰æ‹©**ç´¯ç§¯æ¦‚ç‡è¶…è¿‡æŸä¸ªé˜ˆå€¼ p çš„æœ€å°é›†åˆï¼Œç„¶åä»è¿™ä¸ªé›†åˆä¸­éšæœºé€‰æ‹©ä¸‹ä¸€ä¸ªè¯**ã€‚è¿™ä¸ªé›†åˆè¢«ç§°ä¸º `nucleus`ï¼Œå³æ ¸å¿ƒï¼Œè¿™ä¹Ÿæ˜¯ `nucleus sampling` åç§°çš„æ¥æºã€‚

ç”¨ä¸€ä¸ªå›¾æ¥å½¢è±¡çš„è§£é‡Šæ•´ä¸ªæµç¨‹ï¼ˆå›¾å³ï¼‰ï¼Œå³å…ˆé€‰æ‹©å‡º nucleus é›†åˆåï¼Œé‡æ–°è¿›è¡Œæ¦‚ç‡çš„å½’ä¸€åŒ–å†è¿›è¡Œé‡‡æ ·ã€‚å¦‚æœä½ è¿˜æ˜¯ä¸å¤ªç†è§£ Top-p sampling å¯ä»¥å‚è€ƒ `llama2` ä¸­çš„ [`sample_top_p`å‡½æ•°å®ç°](https://github.com/meta-llama/llama/blob/main/llama/generation.py#L398-L421)ã€‚

![Process-of-top-k-and-top-p-sampling](./assets/Process-of-top-k-and-top-p-sampling.png)

## 2. Learn by `llama` code 

### 2.1. `generate` function signature

ç½‘ä¸Šè‚¯å®šæœ‰è®¸å¤šå…³äº `llama` åº“æ•´ä¸ªä»£ç çš„è®²è§£ã€‚ä¸ºäº†å·®å¼‚åŒ–ï¼Œè¿™é‡Œä»…é‡ç‚¹ä»‹ç»å’Œä¸»é¢˜æœ‰å…³çš„ `generate` å‡½æ•°ã€‚ï¼ˆå¹¶ä¸”ä¼šç•¥å»ä¸€äº›ä¸å½±å“å™è¿° generate é€»è¾‘çš„éƒ¨åˆ†ï¼‰å¹¶ä¸ä¼šåœ¨æ–‡ç« å†…é€è¡Œçš„è§£é‡Šä»£ç ï¼ˆä¼šåœ¨ repo ä¸­æ”¾ä¸€ä¸ªæ³¨é‡Šç‰ˆæœ¬ï¼‰ã€‚

é¦–å…ˆæµè§ˆå‡½æ•°ç­¾åï¼š

- [`torch.inference_mode()`](https://pytorch.org/docs/stable/generated/torch.inference_mode.html)ï¼š å¯ä»¥ç†è§£ä¸ºä¸ `torch.no_grad()` ç±»ä¼¼çš„ä¼˜åŒ–ï¼Œç”¨äºåŠ é€Ÿæ¨ç†ã€‚
- `prompt_tokens`ï¼šäºŒç»´åˆ—è¡¨ç”¨æ¥å­˜å‚¨ prompts tokenized åçš„ tokens idã€‚å…¶ä¸­ç¬¬ä¸€ç»´ä»£è¡¨çš„æ˜¯ batch sizeã€‚
- `max_gen_len`ï¼šç”Ÿæˆçš„æ–‡æœ¬åºåˆ—çš„æœ€å¤§é•¿åº¦ã€‚
- `temperature`ï¼šå‚è€ƒå‰æ–‡ä¸­çš„ä»‹ç»ã€‚ç”¨æ¥æ§åˆ¶é‡‡æ ·çš„éšæœºæ€§ã€‚
- `top_p`ï¼šåŒæ ·å‚è€ƒå‰æ–‡ä¸­çš„ä»‹ç»ã€‚ç”¨äºè®¾ç½® top-p sampling çš„é˜ˆå€¼ã€‚
- å…¶ä»–çš„å‚æ•°æˆ‘ä»¬å¯ä»¥ skip æ‰ï¼Œåœ¨æœ¬æ–‡ä¸­å¹¶ä¸é‡è¦ã€‚

```python
@torch.inference_mode()
def generate(
    self,
    prompt_tokens: List[List[int]],
    max_gen_len: int,
    temperature: float = 0.6,
    top_p: float = 0.9,
    logprobs: bool = False,
    echo: bool = False,
) -> Tuple[List[List[int]], Optional[List[List[float]]]]:
```

### 2.2. Goal1: How to auto-regressive sampling?

å‡è®¾æˆ‘ä»¬æœ‰ $N$ ä¸ª tokensï¼š

1. ç¬¬ä¸€æ­¥ï¼Œ $N$ ä¸ª prompt tokens åŒæ—¶è¾“å…¥åˆ°æ¨¡å‹ï¼Œå¹¶å¾—åˆ° $N$ ä¸ª distributionï¼ˆç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªtokençš„ï¼‰ï¼Œä»…æœ€åä¸€ä¸ªä¸ºæˆ‘ä»¬éœ€è¦çš„åˆ†å¸ƒã€‚ï¼ˆ`å›¾ Step1 å³ä¾§`ï¼‰
2. åŒæ—¶ï¼Œç¬¬ä¸€æ¬¡è¾“å…¥åˆ°æ¨¡å‹ä¸­çš„ prompt tokens çš„è®¡ç®—ç»“æœä¼šè¢«ç¼“å­˜åˆ° kv cache ä¸­ï¼ˆ`å›¾ Step2 å·¦ä¾§ç°è‰²å—`ï¼‰ï¼Œå› æ­¤ä¹‹åçš„è‡ªå›å½’é‡‡æ ·åªéœ€è¦è¾“å…¥ä¸Šä¸€æ¬¡é¢„æµ‹å¾—åˆ°çš„ tokenã€‚
3. é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´åˆ°å¾—åˆ°**ç»“æŸ token**æˆ–è€…è¶…è¿‡æ¨¡å‹è®¾å®šçš„**æœ€å¤§åºåˆ—é•¿åº¦**ã€‚

<img src="./assets/auto-regressive-sampling.png" alt="auto-regressive-sampling" style="zoom:25%;" />

### 2.3. Goal2: How to batch inference?

[[TODO]]

## References

- [Blog: LLM Inferenceä¸²è®²](https://xv44586.github.io/2023/03/10/llm-inf/index.html)
- [Github Repo: meta-llama/llama](https://github.com/meta-llama/llama)
- [TORCH.MULTINOMIAL](https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial)
- [Blog: 2023å¹´çš„æ·±åº¦å­¦ä¹ å…¥é—¨æŒ‡å—(19) - LLaMA 2æºç è§£æ](https://juejin.cn/post/7259738325031944247)

## Next Read

[[TODO]]