# Inference Optimization: TensorRT ç³»åˆ—

é¦–å…ˆåæ§½ä¸‹ï¼ŒNVIDIA çš„æ–‡æ¡£è¯»èµ·æ¥ä¸æ˜¯å¾ˆå‹å¥½ã€‚ï¼ˆæ„Ÿè§‰é…è‰²æœ‰ç‚¹æ€ªï¼‰

TensorRT çš„å…·ä½“å®ç°è¿‡äºåº•å±‚ï¼Œæˆ‘çš„èƒ½åŠ›æœ‰é™ï¼Œæ— æ³•è¿›è¡Œè¯¦ç»†çš„æŠ€æœ¯è§’åº¦åˆ†æã€‚ä¸»è¦ä» User çš„è§’åº¦æ¥ç®€å•çš„ä½¿ç”¨åˆ†æã€‚

## TensorRT

### æ˜¯ä»€ä¹ˆï¼Ÿ

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦äº†è§£ä»€ä¹ˆæ˜¯ TensorRTï¼ŒTensorRT æ˜¯ NVIDIA æ¨å‡ºçš„ä¸€æ¬¾é«˜æ€§èƒ½æ·±åº¦å­¦ä¹ æ¨ç†SDKã€‚æ­¤ SDK åŒ…å«æ·±åº¦å­¦ä¹ æ¨ç†ä¼˜åŒ–å™¨å’Œè¿è¡Œç¯å¢ƒ,å¯ä¸ºæ·±åº¦å­¦ä¹ æ¨ç†åº”ç”¨æä¾›ä½å»¶è¿Ÿå’Œé«˜ååé‡ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ä¸ªç®€çŸ­çš„ example æ¥è¯´æ˜å®ƒçš„ç”¨å¤„ã€‚è¿™é‡Œä¸»è¦æ˜¯åŸºäº NVIDIA å®˜æ–¹æä¾›çš„ ONNX ä½¿ç”¨ Exampleã€‚

![tensorrt-onnx](./assets/tensorrt-onnx.png)

TensorRT å°±æ˜¯é’ˆå¯¹ä¸åŒçš„åç«¯ï¼Œå»ç»§ç»­ç½‘ç»œç»“æ„ç­‰å„ç§ä¼˜åŒ–ï¼Œæœ€ååŠ é€Ÿæ¨ç†æ€§èƒ½ã€‚æ›´è¿›ä¸€æ­¥æè¿°ï¼šâ€œ**æ ¹æ®ç½‘ç»œç»“æ„ã€è¾“å…¥ã€è¾“å‡ºtensorã€ç›®æ ‡GPUçš„èµ„æºï¼Œé€šè¿‡å®é™…è¿è¡Œï¼Œåœ¨å€™é€‰Kernelåº“ä¸­æ‹©ä¼˜çš„ä¸€ä¸ªHardware Awareä¼˜åŒ–å™¨ã€‚**â€

### ä¼˜åŒ–æŠ€æœ¯

TensorRT æ˜¯åŠå¼€æºçš„ï¼Œè¿™äº›æ ¸å¿ƒçš„ä¼˜åŒ–å¹¶æ²¡æœ‰å®é™…å¼€æºã€‚

![tensorrt-optimizations](./assets/tensorrt-optimizations.png)

### æ¡†æ¶ä¼˜åŠ¿

TensorRT çš„ä¼˜åŠ¿ä¸»è¦æ˜¯ï¼š

- æ¨ç†é€Ÿåº¦æå‡ã€‚
- æ”¯æŒå¤šç§æ¨¡å‹é‡åŒ–ã€‚
- **å¤šåº”ç”¨ï¼Œå¤šå¡æ¨ç†ï¼›å•å¡å¤šæµæ¨ç†**ã€‚

![tensorrt-advantages](./assets/tensorrt-advantages.png)

## TensorRT-LLM

### æ˜¯ä»€ä¹ˆï¼Ÿ

> NVIDIA TensorRT-LLM æ˜¯ä¸€ä¸ªå¼€æºåº“ï¼Œå¯åŠ é€Ÿå’Œä¼˜åŒ– NVIDIA AI å¹³å°ä¸Šæœ€æ–°å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) çš„æ¨ç†æ€§èƒ½ã€‚å®ƒè®©å¼€å‘äººå‘˜å¯ä»¥å°è¯•æ–°çš„ LLMï¼Œæä¾›é«˜æ€§èƒ½å’Œå¿«é€Ÿå®šåˆ¶ï¼Œè€Œæ— éœ€æ·±å…¥äº†è§£ C++ æˆ– CUDAã€‚
>
> TensorRT-LLM is a toolkit to assemble optimized solutions to perform Large Language Model (LLM) inference. It offers a Python API to define models and compile efficient [TensorRT](https://developer.nvidia.com/tensorrt) engines for NVIDIA GPUs. It also contains Python and C++ components to build runtimes to execute those engines as well as backends for the [Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server) to easily create web-based services for LLMs. TensorRT-LLM supports **multi-GPU and multi-node configurations (through MPI).**
>
> TensorRT-LLM æ˜¯åœ¨ TensorRT åŸºç¡€ä¸Šé’ˆå¯¹å¤§æ¨¡å‹è¿›ä¸€æ­¥ä¼˜åŒ–çš„åŠ é€Ÿæ¨ç†åº“ï¼Œå®ƒå·ç§°å¯ä»¥**å¢åŠ  4 å€**çš„æ¨ç†é€Ÿåº¦ã€‚

### ä¸ºä»€ä¹ˆï¼Ÿï¼ˆä¸ºä»€ä¹ˆè¦åœ¨æœ‰ TensorRT çš„æƒ…å†µä¸‹æ¨å‡º TensorRT-LLMï¼‰

TensorRT-LLM çš„æ¨å‡ºæ˜¯ä¸ºäº†æ»¡è¶³åœ¨ç°ä»£AIåº”ç”¨ä¸­å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹é«˜æ•ˆæ¨ç†çš„éœ€æ±‚ã€‚éšç€æ¨¡å‹å°ºå¯¸çš„å¢é•¿ï¼Œå¦‚ GPT-3 å’Œ BERTï¼Œä»¥åŠç”¨ä¾‹çš„å¤æ‚æ€§å¢åŠ ï¼Œå¯¹äºä¸€ä¸ªèƒ½å¤Ÿå¿«é€Ÿã€æœ‰æ•ˆåœ°æ‰§è¡Œè¿™äº›æ¨¡å‹çš„æ¨ç†å¼•æ“çš„éœ€æ±‚ä¹Ÿåœ¨å¢é•¿ã€‚ä¼ ç»Ÿæ¨ç†å·¥å…·å¯èƒ½éš¾ä»¥å¤„ç†å¦‚æ­¤åºå¤§å’Œå¤æ‚çš„æ¨¡å‹ï¼Œè€Œ TensorRT-LLM é€šè¿‡æä¾›ä¸“é—¨çš„ä¼˜åŒ–å’Œé«˜çº§è°ƒåº¦æŠ€æœ¯ï¼Œèƒ½å¤Ÿæé«˜æ€§èƒ½ï¼Œé™ä½èƒ½è€—å’Œæ€»ä½“æˆæœ¬ã€‚

æ‰€ä»¥ï¼Œç»¼åˆçœ‹æ¥ TensorRT-LLM çš„æ¨å‡ºæ˜¯ä¸ºäº†**è¿æ¥ Generative AI çˆ†å‘å¼å¢é•¿åšçš„é’ˆå¯¹æ€§ä¼˜åŒ–**ã€‚

### æ¡†æ¶ä¼˜åŠ¿

æŒ‘ä¸€äº›å¯¹ä¸ªäººç”¨æˆ·æ¯”è¾ƒç›´è§‚çš„ä¼˜åŠ¿ï¼š

- **æ€§èƒ½æå‡**ï¼šæå‡ LLM çš„æ¨ç†é€Ÿåº¦ï¼Œå¯ä»¥æä¾›å¤šè¾¾ 4 å€å¤šçš„æ€§æå‡[[â€]](https://www.techrepublic.com/article/nvidia-announces-tensorrt-llm/)ã€‚
- **æ˜“äºä½¿ç”¨**ï¼šæä¾›äº† Python API ï¼Œä½¿å¾—å¼€å‘è€…å¯ä»¥æ›´å¿«åœ°åˆ›å»ºå’Œéƒ¨ç½²å®šåˆ¶çš„ LLM åº”ç”¨ï¼Œè€Œæ— éœ€æ·±å…¥äº†è§£åº•å±‚æŠ€æœ¯ã€‚
- **å¤šèŠ‚ç‚¹æ”¯æŒ**ï¼šæ”¯æŒå¤š GPU å’Œè·¨æœåŠ¡å™¨çš„æ¨¡å‹æ¨ç†ï¼Œä½¿å¾—å®ƒå¯ä»¥åœ¨å¤§è§„æ¨¡çš„åŸºç¡€è®¾æ–½ä¸Šè¿è¡Œã€‚ï¼ˆå…¶å®æ„Ÿè§‰æ˜¯å› ä¸º TensorRT æœ¬èº«å°±æ”¯æŒï¼‰ã€‚

æœ€é‡è¦çš„æ˜¯ï¼ŒTensorRT-LLM **æå¤§åœ°ç®€åŒ–äº†å¼€å‘æµç¨‹**ï¼Œä½¿å¾—å¼€å‘è€…æ— éœ€æ·±å…¥äº†è§£åº•å±‚çš„æŠ€æœ¯ç»†èŠ‚ï¼Œä¹Ÿæ— éœ€ç¼–å†™å¤æ‚çš„ CUDA/C++ ä»£ç ã€‚æ€»çš„æ¥è¯´ï¼ŒTensorRT-LLM è®©ç”¨æˆ·å¯ä»¥ä¸“æ³¨äºæ¨¡å‹çš„è®¾è®¡å’Œä¼˜åŒ–ï¼Œè€Œå°†åº•å±‚çš„æ€§èƒ½ä¼˜åŒ–å·¥ä½œäº¤ç»™ TensorRT æ¥å®Œæˆï¼Œå¤§å¤§æé«˜äº†å¼€å‘æ•ˆç‡å’Œç”Ÿäº§æ•ˆç‡ï¼ŒçœŸæ­£å®ç°äº†å¤§æ¨¡å‹æ¨ç†çš„æ˜“ç”¨æ€§å’Œé«˜æ•ˆæ€§ã€‚

### æ€§èƒ½å¯¹æ¯”ï¼ˆå¯¹æ¯”å…¶ä»–çš„åŒç±»å‹çš„æ¡†æ¶ä¾‹å¦‚ vLLMï¼‰

ç›®å‰å¹¶æ²¡æœ‰å®˜æ–¹å…¬å¼€çš„å¯¹æ¯”æ•°æ®ã€‚ä¾æ® Github ä¸Š NVIDIA å·¥ä½œäººå‘˜çš„å‘è¨€ï¼š

> We do not plan to publish performance numbers that compare TensorRT-LLM with vLLM.
>
> Our internal measurements show that TensorRT-LLMâ€™s in-flight batching and paged KV cache features work well and TensorRT-LLM can deliver great performance. Weâ€™d be happy to provide you with performance numbers for relevant cases.
>
> Is there a particular use case that youâ€™d be interested in?

æˆ‘è®¤ä¸º TensorRT-LLM å¯¹æ¯” vLLM **åº”è¯¥æ²¡æœ‰å·¨å¤§çš„æ€§èƒ½æå‡**ã€‚æ›´ä¸ºå¯é çš„è¯æ®æºè‡ªæŸä¸ªæ¨ç‰¹ [[â€]](https://twitter.com/HamelHusain/status/1719872352694174093) ï¼Œè¯¥æ¨ç‰¹å†…å®¹è¯´æ˜åœ¨åŒæ ·ä½¿ç”¨ triton server è¿›è¡Œ Llama çš„ inference æƒ…å†µä¸‹ï¼ŒvLLM çš„æ€§èƒ½æ¯” TensorRT æ›´ä¼˜ã€‚

### æ¡†æ¶ä½¿ç”¨ï¼ˆCode Exampleï¼Œå®Œæ•´çš„é€‚é…æ¨¡å‹çš„è¿‡ç¨‹ï¼‰

è¿™é‡Œæœ‰ä¸€ä¸ª TensorRT-LLM ä¼˜åŒ– llama çš„ Exampleã€‚

```
# Build the LLaMA 7B model using a single GPU and FP16.
python build.py --model_dir ./tmp/llama/7B/ \
                --dtype float16 \
                --remove_input_padding \
                --use_gpt_attention_plugin float16 \
                --enable_context_fmha \
                --use_gemm_plugin float16 \
                --output_dir ./tmp/llama/7B/trt_engines/fp16/1-gpu/
# With fp16 inference
python3 ../run.py --max_output_len=50 \
                  --tokenizer_dir ./tmp/llama/7B/ \
                  --engine_dir=./tmp/llama/7B/trt_engines/fp16/1-gpu/
```

## vLLM ä¸ TensorRT-LLM çš„é€‰æ‹©

ç»¼åˆæ¥è¯´ï¼ŒvLLM å’Œ TensorRT éƒ½æ˜¯å¯ä»¥å’Œ triton server ä¸€èµ·ä½¿ç”¨è¿›è¡Œæ¨¡å‹éƒ¨ç½²ã€‚è€Œä»–ä»¬ä¸¤è€…ä¹‹é—´çš„é€‰æ‹©ï¼Œå¯ä»¥å‚è€ƒï¼š

- vLLM æ¯” TensorRT-LLM å‘å¸ƒçš„æ—¶é—´æ›´ä¹…ï¼Œ**ç›¸å…³æŠ€æœ¯åšå®¢æ›´å¤š**ã€‚vLLM å·²ç»è¢«åŒ…æ‹¬ LMSYS Vicuna å’Œ Chatbot Arena åœ¨å†…çš„å¹³å°ç”¨æ¥æ”¯æŒå…¶è‡ªä» 2023 å¹´ 4 æœˆä»½çš„è¿è¡Œï¼Œè¿™ä¹Ÿè¡¨æ˜äº† vLLM é¡¹ç›®çš„å®ç”¨æ€§å’Œç¨³å®šæ€§ã€‚TensorRT-LLM æ˜¯23å¹´9æœˆå‘å¸ƒçš„ï¼Œ**ç½‘ç»œä¸Šçš„ç›¸å…³èµ„æ–™ä¸å¤š**ã€‚
- å¯¹æ¯”æ„é€  inference engine çš„ä»£ç ï¼Œ[TensorRT-LLM build llama](https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/llama/build.py) æ¯”èµ· [vLLM éƒ¨ç½² llama](https://zhuanlan.zhihu.com/p/645732302) **è¦å¤æ‚å¾ˆå¤š**ã€‚
- TensorRT-LLMï¼ŒCUDA å’Œ Triton Server éƒ½æ˜¯ NV çš„äº§å“ï¼Œå¯èƒ½è”åŠ¨ä¸ä¼˜åŒ–æ•ˆæœæ›´å¥½ã€‚å¹¶ä¸”è™½ç„¶ TensorRT-LLM æ›´ä¸ºå¤æ‚ï¼Œ**ä½†æ˜¯ä»æè‡´ä¼˜åŒ–å’Œå¯å®šåˆ¶æ€§çš„è§’åº¦ TensorRT-LLM çš„ä¸Šé™å¯èƒ½æ›´é«˜**ã€‚

ä»æ˜“ç”¨æ€§çš„è§’åº¦å¯ä»¥é€‰æ‹© vLLMï¼Œä»æ€§èƒ½å’Œæ‰©å±•æ€§çš„è§’åº¦å‡ºå‘å¯ä»¥é€‰æ‹© TensorRT-LLMã€‚

## å®æˆ˜

- [ ] TODO

## Reference

- [NVIDIA: TensorRT](https://developer.nvidia.cn/tensorrt)
- [NVIDAI: Optimizing Inference on Large Language Models with NVIDIA TensorRT-LLM, Now Publicly Available](https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/)
- [:desktop_computer: Github, TensorRT-LLM Llama example](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama)
- [TensorRTæ˜¯å¦‚ä½•åšåˆ°æ¯”å…¶ä»–æ¡†æ¶æ›´å¿«çš„ï¼Ÿ-- çŸ¥ä¹è¿˜æ²¡äººè°ˆåˆ°çš„å†…æ ¸éƒ¨åˆ†](https://zhuanlan.zhihu.com/p/666638357)
- [vllm vs TGI éƒ¨ç½² llama v2 7B è¸©å‘ç¬”è®°](https://zhuanlan.zhihu.com/p/645732302)
- [ğŸ”¥Continuous Batchingï¼šä¸€ç§æå‡ LLM éƒ¨ç½²ååé‡çš„åˆ©å™¨ (vLLM å’Œ TensorRT éƒ½ç”¨åˆ°äº†)](https://zhuanlan.zhihu.com/p/657586838)